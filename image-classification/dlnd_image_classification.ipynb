{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f11d46ed7f0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement Function\n",
    "    xmax,xmin = x.max(),x.min()\n",
    "    return (x - xmin)/(xmax - xmin)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    return np.eye(10)[x]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x = tf.placeholder(tf.float32, [None,image_shape[0],image_shape[1],image_shape[2]],name=\"x\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes],name=\"y\")\n",
    "    return y\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    keep_prob = tf.placeholder(tf.float32,name=\"keep_prob\")\n",
    "    return keep_prob\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weights =  tf.Variable(tf.truncated_normal(\n",
    "        (conv_ksize[0], conv_ksize[1], x_tensor.get_shape().as_list()[-1], conv_num_outputs), stddev=0.1))\n",
    "    #weights = tf.placeholder(tf.float32,[conv_ksize[0],conv_ksize[1], x_tensor.shape[3] , conv_num_outputs]);\n",
    "    #bias = tf.placeholder(tf.float32,[conv_num_outputs])\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    conv = tf.nn.conv2d(x_tensor, weights, strides=[1,conv_strides[0],conv_strides[1],1], padding='SAME')\n",
    "    conv = tf.nn.bias_add(conv, bias)\n",
    "    \n",
    "    return  tf.nn.max_pool(conv, ksize=[1,pool_ksize[0],pool_ksize[1],1], strides=[1,pool_strides[0],pool_strides[1],1],\n",
    "                          padding='SAME')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement Function\n",
    "    x_shape = x_tensor.get_shape().as_list()\n",
    "    return tf.reshape(x_tensor,shape=[-1,x_shape[1]*x_shape[2]*x_shape[3]])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #weights = tf.placeholder(tf.float32,[x_tensor.shape[1] , num_outputs])\n",
    "    weights =  tf.Variable(tf.truncated_normal(\n",
    "        (x_tensor.get_shape().as_list()[1], num_outputs), stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    return tf.nn.relu(tf.add(tf.matmul(x_tensor, weights), bias))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weights =  tf.Variable(tf.truncated_normal(\n",
    "        (x_tensor.get_shape().as_list()[1], num_outputs),  stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    return tf.add(tf.matmul(x_tensor, weights), bias)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_ksize = [\n",
    "        [3,3],\n",
    "        [3,3],\n",
    "        [3,3]\n",
    "    ]\n",
    "    conv_strids = [\n",
    "        [1,1],\n",
    "        [2,2],\n",
    "        [1,1]\n",
    "    ]\n",
    "    \n",
    "    pool_ksize = [\n",
    "        [5,5],\n",
    "        [4,4],\n",
    "        [2,2]\n",
    "    ]\n",
    "    \n",
    "    pool_strides = [\n",
    "        [2,2],\n",
    "        [1,1],\n",
    "        [1,1]\n",
    "    ]\n",
    "    \n",
    "    conv_num_outputs = [32,32,16]\n",
    "    \n",
    "    num_outputs = [256,128]\n",
    "    \n",
    "    n_class = 10\n",
    "  \n",
    "    conv0 = x\n",
    "    conv1 = conv2d_maxpool(conv0, conv_num_outputs[0] , conv_ksize[0] , conv_strids[0] , pool_ksize[0] , pool_strides[0])\n",
    "    conv2 = conv2d_maxpool(conv1, conv_num_outputs[1] , conv_ksize[1] , conv_strids[1] , pool_ksize[1] , pool_strides[1])\n",
    "    conv3 = conv2d_maxpool(conv2, conv_num_outputs[2] , conv_ksize[2] , conv_strids[2] , pool_ksize[2] , pool_strides[2])\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    fc0 = flatten(conv3)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    fc1 = fully_conn(fc0,num_outputs[0])\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    \n",
    "    fc2 = fully_conn(fc1,num_outputs[1])\n",
    "    fc2 = tf.nn.dropout(fc2, keep_prob)\n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    out = output(fc2,n_class)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    session.run(optimizer,feed_dict={x:feature_batch,y:label_batch,keep_prob:keep_probability})\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost    ,feed_dict = {x:feature_batch,y:label_batch,keep_prob:1.0})\n",
    "    acc  = session.run(accuracy,feed_dict = {x:feature_batch,y:label_batch,keep_prob:1.0})\n",
    "    print('For the train data Loss: %f Acc: %f'%(loss, acc))\n",
    "    valid_loss = session.run(cost    ,feed_dict = {x:valid_features,y:valid_labels,keep_prob:1.0})\n",
    "    valid_acc  = session.run(accuracy,feed_dict = {x:valid_features,y:valid_labels,keep_prob:1.0})\n",
    "    print('For the validation data Loss: %f Acc: %f'%(valid_loss, valid_acc))\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "keep_probability = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  For the train data Loss: 1.841639 Acc: 0.425000\n",
      "For the validation data Loss: 1.722850 Acc: 0.377200\n",
      "Epoch  2, CIFAR-10 Batch 1:  For the train data Loss: 1.594698 Acc: 0.575000\n",
      "For the validation data Loss: 1.542040 Acc: 0.431400\n",
      "Epoch  3, CIFAR-10 Batch 1:  For the train data Loss: 1.418483 Acc: 0.500000\n",
      "For the validation data Loss: 1.447388 Acc: 0.468200\n",
      "Epoch  4, CIFAR-10 Batch 1:  For the train data Loss: 1.246547 Acc: 0.675000\n",
      "For the validation data Loss: 1.366560 Acc: 0.503400\n",
      "Epoch  5, CIFAR-10 Batch 1:  For the train data Loss: 1.104471 Acc: 0.675000\n",
      "For the validation data Loss: 1.300047 Acc: 0.530000\n",
      "Epoch  6, CIFAR-10 Batch 1:  For the train data Loss: 1.014907 Acc: 0.625000\n",
      "For the validation data Loss: 1.303136 Acc: 0.537000\n",
      "Epoch  7, CIFAR-10 Batch 1:  For the train data Loss: 0.908947 Acc: 0.750000\n",
      "For the validation data Loss: 1.251360 Acc: 0.555600\n",
      "Epoch  8, CIFAR-10 Batch 1:  For the train data Loss: 0.873761 Acc: 0.750000\n",
      "For the validation data Loss: 1.233520 Acc: 0.563200\n",
      "Epoch  9, CIFAR-10 Batch 1:  For the train data Loss: 0.827197 Acc: 0.650000\n",
      "For the validation data Loss: 1.204159 Acc: 0.568400\n",
      "Epoch 10, CIFAR-10 Batch 1:  For the train data Loss: 0.773936 Acc: 0.725000\n",
      "For the validation data Loss: 1.227361 Acc: 0.573600\n",
      "Epoch 11, CIFAR-10 Batch 1:  For the train data Loss: 0.721337 Acc: 0.775000\n",
      "For the validation data Loss: 1.194899 Acc: 0.586000\n",
      "Epoch 12, CIFAR-10 Batch 1:  For the train data Loss: 0.769013 Acc: 0.650000\n",
      "For the validation data Loss: 1.188310 Acc: 0.590200\n",
      "Epoch 13, CIFAR-10 Batch 1:  For the train data Loss: 0.672592 Acc: 0.675000\n",
      "For the validation data Loss: 1.195825 Acc: 0.593400\n",
      "Epoch 14, CIFAR-10 Batch 1:  For the train data Loss: 0.690218 Acc: 0.700000\n",
      "For the validation data Loss: 1.152330 Acc: 0.606000\n",
      "Epoch 15, CIFAR-10 Batch 1:  For the train data Loss: 0.621731 Acc: 0.750000\n",
      "For the validation data Loss: 1.160881 Acc: 0.606800\n",
      "Epoch 16, CIFAR-10 Batch 1:  For the train data Loss: 0.590688 Acc: 0.750000\n",
      "For the validation data Loss: 1.181239 Acc: 0.610200\n",
      "Epoch 17, CIFAR-10 Batch 1:  For the train data Loss: 0.495406 Acc: 0.800000\n",
      "For the validation data Loss: 1.232307 Acc: 0.603600\n",
      "Epoch 18, CIFAR-10 Batch 1:  For the train data Loss: 0.468363 Acc: 0.850000\n",
      "For the validation data Loss: 1.183874 Acc: 0.610200\n",
      "Epoch 19, CIFAR-10 Batch 1:  For the train data Loss: 0.451247 Acc: 0.825000\n",
      "For the validation data Loss: 1.221159 Acc: 0.603200\n",
      "Epoch 20, CIFAR-10 Batch 1:  For the train data Loss: 0.440075 Acc: 0.825000\n",
      "For the validation data Loss: 1.205728 Acc: 0.613800\n",
      "Epoch 21, CIFAR-10 Batch 1:  For the train data Loss: 0.439023 Acc: 0.850000\n",
      "For the validation data Loss: 1.201794 Acc: 0.622200\n",
      "Epoch 22, CIFAR-10 Batch 1:  For the train data Loss: 0.363546 Acc: 0.875000\n",
      "For the validation data Loss: 1.204536 Acc: 0.612200\n",
      "Epoch 23, CIFAR-10 Batch 1:  For the train data Loss: 0.444876 Acc: 0.775000\n",
      "For the validation data Loss: 1.172702 Acc: 0.621200\n",
      "Epoch 24, CIFAR-10 Batch 1:  For the train data Loss: 0.412541 Acc: 0.825000\n",
      "For the validation data Loss: 1.246336 Acc: 0.617600\n",
      "Epoch 25, CIFAR-10 Batch 1:  For the train data Loss: 0.326590 Acc: 0.875000\n",
      "For the validation data Loss: 1.257339 Acc: 0.613800\n",
      "Epoch 26, CIFAR-10 Batch 1:  For the train data Loss: 0.279199 Acc: 0.875000\n",
      "For the validation data Loss: 1.329707 Acc: 0.606200\n",
      "Epoch 27, CIFAR-10 Batch 1:  For the train data Loss: 0.296926 Acc: 0.925000\n",
      "For the validation data Loss: 1.282139 Acc: 0.610000\n",
      "Epoch 28, CIFAR-10 Batch 1:  For the train data Loss: 0.242373 Acc: 0.950000\n",
      "For the validation data Loss: 1.310318 Acc: 0.610000\n",
      "Epoch 29, CIFAR-10 Batch 1:  For the train data Loss: 0.255951 Acc: 0.900000\n",
      "For the validation data Loss: 1.349265 Acc: 0.600600\n",
      "Epoch 30, CIFAR-10 Batch 1:  For the train data Loss: 0.271864 Acc: 0.950000\n",
      "For the validation data Loss: 1.390230 Acc: 0.610800\n",
      "Epoch 31, CIFAR-10 Batch 1:  For the train data Loss: 0.243079 Acc: 0.950000\n",
      "For the validation data Loss: 1.357175 Acc: 0.614600\n",
      "Epoch 32, CIFAR-10 Batch 1:  For the train data Loss: 0.200625 Acc: 0.925000\n",
      "For the validation data Loss: 1.398777 Acc: 0.614000\n",
      "Epoch 33, CIFAR-10 Batch 1:  For the train data Loss: 0.208035 Acc: 0.950000\n",
      "For the validation data Loss: 1.445458 Acc: 0.608000\n",
      "Epoch 34, CIFAR-10 Batch 1:  For the train data Loss: 0.189304 Acc: 0.975000\n",
      "For the validation data Loss: 1.407235 Acc: 0.606600\n",
      "Epoch 35, CIFAR-10 Batch 1:  For the train data Loss: 0.244829 Acc: 0.875000\n",
      "For the validation data Loss: 1.458957 Acc: 0.610000\n",
      "Epoch 36, CIFAR-10 Batch 1:  For the train data Loss: 0.212919 Acc: 0.900000\n",
      "For the validation data Loss: 1.476883 Acc: 0.618400\n",
      "Epoch 37, CIFAR-10 Batch 1:  For the train data Loss: 0.147289 Acc: 1.000000\n",
      "For the validation data Loss: 1.463262 Acc: 0.621800\n",
      "Epoch 38, CIFAR-10 Batch 1:  For the train data Loss: 0.134939 Acc: 0.950000\n",
      "For the validation data Loss: 1.524515 Acc: 0.622000\n",
      "Epoch 39, CIFAR-10 Batch 1:  For the train data Loss: 0.144818 Acc: 0.950000\n",
      "For the validation data Loss: 1.468090 Acc: 0.613800\n",
      "Epoch 40, CIFAR-10 Batch 1:  For the train data Loss: 0.196685 Acc: 0.925000\n",
      "For the validation data Loss: 1.479214 Acc: 0.614800\n",
      "Epoch 41, CIFAR-10 Batch 1:  For the train data Loss: 0.133794 Acc: 0.975000\n",
      "For the validation data Loss: 1.581960 Acc: 0.612000\n",
      "Epoch 42, CIFAR-10 Batch 1:  For the train data Loss: 0.128730 Acc: 0.975000\n",
      "For the validation data Loss: 1.535336 Acc: 0.619600\n",
      "Epoch 43, CIFAR-10 Batch 1:  For the train data Loss: 0.091605 Acc: 1.000000\n",
      "For the validation data Loss: 1.701858 Acc: 0.600200\n",
      "Epoch 44, CIFAR-10 Batch 1:  For the train data Loss: 0.121758 Acc: 0.975000\n",
      "For the validation data Loss: 1.641747 Acc: 0.610600\n",
      "Epoch 45, CIFAR-10 Batch 1:  For the train data Loss: 0.155087 Acc: 0.925000\n",
      "For the validation data Loss: 1.612221 Acc: 0.618600\n",
      "Epoch 46, CIFAR-10 Batch 1:  For the train data Loss: 0.083414 Acc: 1.000000\n",
      "For the validation data Loss: 1.650327 Acc: 0.619000\n",
      "Epoch 47, CIFAR-10 Batch 1:  For the train data Loss: 0.080463 Acc: 1.000000\n",
      "For the validation data Loss: 1.804978 Acc: 0.612800\n",
      "Epoch 48, CIFAR-10 Batch 1:  For the train data Loss: 0.071251 Acc: 1.000000\n",
      "For the validation data Loss: 1.744396 Acc: 0.619000\n",
      "Epoch 49, CIFAR-10 Batch 1:  For the train data Loss: 0.086506 Acc: 1.000000\n",
      "For the validation data Loss: 1.811371 Acc: 0.604800\n",
      "Epoch 50, CIFAR-10 Batch 1:  For the train data Loss: 0.098928 Acc: 0.975000\n",
      "For the validation data Loss: 1.828947 Acc: 0.611200\n",
      "Epoch 51, CIFAR-10 Batch 1:  For the train data Loss: 0.046383 Acc: 1.000000\n",
      "For the validation data Loss: 1.833471 Acc: 0.613200\n",
      "Epoch 52, CIFAR-10 Batch 1:  For the train data Loss: 0.065586 Acc: 0.975000\n",
      "For the validation data Loss: 1.839885 Acc: 0.608800\n",
      "Epoch 53, CIFAR-10 Batch 1:  For the train data Loss: 0.055870 Acc: 1.000000\n",
      "For the validation data Loss: 1.827862 Acc: 0.613000\n",
      "Epoch 54, CIFAR-10 Batch 1:  For the train data Loss: 0.081395 Acc: 1.000000\n",
      "For the validation data Loss: 1.861899 Acc: 0.623200\n",
      "Epoch 55, CIFAR-10 Batch 1:  For the train data Loss: 0.062806 Acc: 0.975000\n",
      "For the validation data Loss: 1.832415 Acc: 0.615400\n",
      "Epoch 56, CIFAR-10 Batch 1:  For the train data Loss: 0.058201 Acc: 1.000000\n",
      "For the validation data Loss: 1.820540 Acc: 0.608800\n",
      "Epoch 57, CIFAR-10 Batch 1:  For the train data Loss: 0.099576 Acc: 0.950000\n",
      "For the validation data Loss: 1.902145 Acc: 0.617800\n",
      "Epoch 58, CIFAR-10 Batch 1:  For the train data Loss: 0.137048 Acc: 0.950000\n",
      "For the validation data Loss: 1.950707 Acc: 0.615000\n",
      "Epoch 59, CIFAR-10 Batch 1:  For the train data Loss: 0.070207 Acc: 0.975000\n",
      "For the validation data Loss: 1.868923 Acc: 0.622200\n",
      "Epoch 60, CIFAR-10 Batch 1:  For the train data Loss: 0.060156 Acc: 1.000000\n",
      "For the validation data Loss: 1.868352 Acc: 0.619800\n",
      "Epoch 61, CIFAR-10 Batch 1:  For the train data Loss: 0.055990 Acc: 1.000000\n",
      "For the validation data Loss: 2.030326 Acc: 0.620800\n",
      "Epoch 62, CIFAR-10 Batch 1:  For the train data Loss: 0.073415 Acc: 0.975000\n",
      "For the validation data Loss: 1.885305 Acc: 0.616600\n",
      "Epoch 63, CIFAR-10 Batch 1:  For the train data Loss: 0.069212 Acc: 1.000000\n",
      "For the validation data Loss: 1.970796 Acc: 0.616400\n",
      "Epoch 64, CIFAR-10 Batch 1:  For the train data Loss: 0.034626 Acc: 1.000000\n",
      "For the validation data Loss: 2.094393 Acc: 0.617400\n",
      "Epoch 65, CIFAR-10 Batch 1:  For the train data Loss: 0.034987 Acc: 1.000000\n",
      "For the validation data Loss: 1.926466 Acc: 0.626400\n",
      "Epoch 66, CIFAR-10 Batch 1:  For the train data Loss: 0.038767 Acc: 1.000000\n",
      "For the validation data Loss: 1.900069 Acc: 0.629600\n",
      "Epoch 67, CIFAR-10 Batch 1:  For the train data Loss: 0.042971 Acc: 1.000000\n",
      "For the validation data Loss: 1.972307 Acc: 0.620200\n",
      "Epoch 68, CIFAR-10 Batch 1:  For the train data Loss: 0.050357 Acc: 1.000000\n",
      "For the validation data Loss: 2.080829 Acc: 0.617400\n",
      "Epoch 69, CIFAR-10 Batch 1:  For the train data Loss: 0.042222 Acc: 1.000000\n",
      "For the validation data Loss: 1.963489 Acc: 0.622600\n",
      "Epoch 70, CIFAR-10 Batch 1:  For the train data Loss: 0.025864 Acc: 1.000000\n",
      "For the validation data Loss: 1.998098 Acc: 0.633800\n",
      "Epoch 71, CIFAR-10 Batch 1:  For the train data Loss: 0.043018 Acc: 1.000000\n",
      "For the validation data Loss: 2.132821 Acc: 0.615200\n",
      "Epoch 72, CIFAR-10 Batch 1:  For the train data Loss: 0.055636 Acc: 1.000000\n",
      "For the validation data Loss: 2.050340 Acc: 0.616400\n",
      "Epoch 73, CIFAR-10 Batch 1:  For the train data Loss: 0.030830 Acc: 1.000000\n",
      "For the validation data Loss: 2.013382 Acc: 0.627600\n",
      "Epoch 74, CIFAR-10 Batch 1:  For the train data Loss: 0.022089 Acc: 1.000000\n",
      "For the validation data Loss: 2.028781 Acc: 0.630600\n",
      "Epoch 75, CIFAR-10 Batch 1:  For the train data Loss: 0.039121 Acc: 1.000000\n",
      "For the validation data Loss: 1.876155 Acc: 0.623200\n",
      "Epoch 76, CIFAR-10 Batch 1:  For the train data Loss: 0.071741 Acc: 0.975000\n",
      "For the validation data Loss: 2.125131 Acc: 0.616000\n",
      "Epoch 77, CIFAR-10 Batch 1:  For the train data Loss: 0.021003 Acc: 1.000000\n",
      "For the validation data Loss: 2.033232 Acc: 0.618800\n",
      "Epoch 78, CIFAR-10 Batch 1:  For the train data Loss: 0.027723 Acc: 1.000000\n",
      "For the validation data Loss: 2.116529 Acc: 0.611000\n",
      "Epoch 79, CIFAR-10 Batch 1:  For the train data Loss: 0.015187 Acc: 1.000000\n",
      "For the validation data Loss: 2.024800 Acc: 0.631200\n",
      "Epoch 80, CIFAR-10 Batch 1:  For the train data Loss: 0.033118 Acc: 0.975000\n",
      "For the validation data Loss: 2.091793 Acc: 0.622000\n",
      "Epoch 81, CIFAR-10 Batch 1:  For the train data Loss: 0.043649 Acc: 0.975000\n",
      "For the validation data Loss: 2.182176 Acc: 0.626200\n",
      "Epoch 82, CIFAR-10 Batch 1:  For the train data Loss: 0.016113 Acc: 1.000000\n",
      "For the validation data Loss: 2.067920 Acc: 0.624800\n",
      "Epoch 83, CIFAR-10 Batch 1:  For the train data Loss: 0.016542 Acc: 1.000000\n",
      "For the validation data Loss: 2.146618 Acc: 0.622800\n",
      "Epoch 84, CIFAR-10 Batch 1:  For the train data Loss: 0.029935 Acc: 1.000000\n",
      "For the validation data Loss: 2.292128 Acc: 0.620200\n",
      "Epoch 85, CIFAR-10 Batch 1:  For the train data Loss: 0.034355 Acc: 1.000000\n",
      "For the validation data Loss: 2.050746 Acc: 0.631000\n",
      "Epoch 86, CIFAR-10 Batch 1:  For the train data Loss: 0.019301 Acc: 1.000000\n",
      "For the validation data Loss: 2.114467 Acc: 0.630600\n",
      "Epoch 87, CIFAR-10 Batch 1:  For the train data Loss: 0.020924 Acc: 1.000000\n",
      "For the validation data Loss: 2.150201 Acc: 0.624400\n",
      "Epoch 88, CIFAR-10 Batch 1:  For the train data Loss: 0.027414 Acc: 1.000000\n",
      "For the validation data Loss: 2.278418 Acc: 0.612800\n",
      "Epoch 89, CIFAR-10 Batch 1:  For the train data Loss: 0.020412 Acc: 1.000000\n",
      "For the validation data Loss: 2.260629 Acc: 0.613000\n",
      "Epoch 90, CIFAR-10 Batch 1:  For the train data Loss: 0.014911 Acc: 1.000000\n",
      "For the validation data Loss: 2.189220 Acc: 0.622200\n",
      "Epoch 91, CIFAR-10 Batch 1:  For the train data Loss: 0.021529 Acc: 1.000000\n",
      "For the validation data Loss: 2.224338 Acc: 0.615400\n",
      "Epoch 92, CIFAR-10 Batch 1:  For the train data Loss: 0.011583 Acc: 1.000000\n",
      "For the validation data Loss: 2.368900 Acc: 0.619200\n",
      "Epoch 93, CIFAR-10 Batch 1:  For the train data Loss: 0.017348 Acc: 1.000000\n",
      "For the validation data Loss: 2.056028 Acc: 0.619600\n",
      "Epoch 94, CIFAR-10 Batch 1:  For the train data Loss: 0.015390 Acc: 1.000000\n",
      "For the validation data Loss: 2.246359 Acc: 0.629200\n",
      "Epoch 95, CIFAR-10 Batch 1:  For the train data Loss: 0.103101 Acc: 0.975000\n",
      "For the validation data Loss: 2.222012 Acc: 0.626400\n",
      "Epoch 96, CIFAR-10 Batch 1:  For the train data Loss: 0.016716 Acc: 1.000000\n",
      "For the validation data Loss: 2.112916 Acc: 0.614000\n",
      "Epoch 97, CIFAR-10 Batch 1:  For the train data Loss: 0.017857 Acc: 1.000000\n",
      "For the validation data Loss: 2.299135 Acc: 0.618600\n",
      "Epoch 98, CIFAR-10 Batch 1:  For the train data Loss: 0.016877 Acc: 1.000000\n",
      "For the validation data Loss: 2.195132 Acc: 0.622800\n",
      "Epoch 99, CIFAR-10 Batch 1:  For the train data Loss: 0.023495 Acc: 1.000000\n",
      "For the validation data Loss: 2.242576 Acc: 0.619400\n",
      "Epoch 100, CIFAR-10 Batch 1:  For the train data Loss: 0.015348 Acc: 1.000000\n",
      "For the validation data Loss: 2.320175 Acc: 0.623200\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  For the train data Loss: 1.924325 Acc: 0.350000\n",
      "For the validation data Loss: 1.744146 Acc: 0.363800\n",
      "Epoch  1, CIFAR-10 Batch 2:  For the train data Loss: 1.631940 Acc: 0.425000\n",
      "For the validation data Loss: 1.552059 Acc: 0.439800\n",
      "Epoch  1, CIFAR-10 Batch 3:  For the train data Loss: 1.433155 Acc: 0.425000\n",
      "For the validation data Loss: 1.446130 Acc: 0.473200\n",
      "Epoch  1, CIFAR-10 Batch 4:  For the train data Loss: 1.307142 Acc: 0.450000\n",
      "For the validation data Loss: 1.330451 Acc: 0.529400\n",
      "Epoch  1, CIFAR-10 Batch 5:  For the train data Loss: 1.371544 Acc: 0.550000\n",
      "For the validation data Loss: 1.298537 Acc: 0.531600\n",
      "Epoch  2, CIFAR-10 Batch 1:  For the train data Loss: 1.354545 Acc: 0.550000\n",
      "For the validation data Loss: 1.281233 Acc: 0.544800\n",
      "Epoch  2, CIFAR-10 Batch 2:  For the train data Loss: 1.267573 Acc: 0.575000\n",
      "For the validation data Loss: 1.209114 Acc: 0.568200\n",
      "Epoch  2, CIFAR-10 Batch 3:  For the train data Loss: 1.048273 Acc: 0.625000\n",
      "For the validation data Loss: 1.181576 Acc: 0.577200\n",
      "Epoch  2, CIFAR-10 Batch 4:  For the train data Loss: 1.110298 Acc: 0.575000\n",
      "For the validation data Loss: 1.195186 Acc: 0.573000\n",
      "Epoch  2, CIFAR-10 Batch 5:  For the train data Loss: 1.132862 Acc: 0.550000\n",
      "For the validation data Loss: 1.126363 Acc: 0.595800\n",
      "Epoch  3, CIFAR-10 Batch 1:  For the train data Loss: 1.125156 Acc: 0.575000\n",
      "For the validation data Loss: 1.131568 Acc: 0.601800\n",
      "Epoch  3, CIFAR-10 Batch 2:  For the train data Loss: 1.176561 Acc: 0.625000\n",
      "For the validation data Loss: 1.110816 Acc: 0.601000\n",
      "Epoch  3, CIFAR-10 Batch 3:  For the train data Loss: 0.885915 Acc: 0.650000\n",
      "For the validation data Loss: 1.090078 Acc: 0.608800\n",
      "Epoch  3, CIFAR-10 Batch 4:  For the train data Loss: 1.032567 Acc: 0.650000\n",
      "For the validation data Loss: 1.066805 Acc: 0.610400\n",
      "Epoch  3, CIFAR-10 Batch 5:  For the train data Loss: 0.965890 Acc: 0.625000\n",
      "For the validation data Loss: 1.065750 Acc: 0.621600\n",
      "Epoch  4, CIFAR-10 Batch 1:  For the train data Loss: 0.958783 Acc: 0.525000\n",
      "For the validation data Loss: 1.080302 Acc: 0.615600\n",
      "Epoch  4, CIFAR-10 Batch 2:  For the train data Loss: 0.970772 Acc: 0.675000\n",
      "For the validation data Loss: 1.021143 Acc: 0.637600\n",
      "Epoch  4, CIFAR-10 Batch 3:  For the train data Loss: 0.847335 Acc: 0.750000\n",
      "For the validation data Loss: 1.035948 Acc: 0.626600\n",
      "Epoch  4, CIFAR-10 Batch 4:  For the train data Loss: 0.902471 Acc: 0.750000\n",
      "For the validation data Loss: 0.997443 Acc: 0.641400\n",
      "Epoch  4, CIFAR-10 Batch 5:  For the train data Loss: 0.866589 Acc: 0.800000\n",
      "For the validation data Loss: 0.990259 Acc: 0.651000\n",
      "Epoch  5, CIFAR-10 Batch 1:  For the train data Loss: 0.856101 Acc: 0.650000\n",
      "For the validation data Loss: 1.027094 Acc: 0.646000\n",
      "Epoch  5, CIFAR-10 Batch 2:  For the train data Loss: 0.910699 Acc: 0.725000\n",
      "For the validation data Loss: 0.968986 Acc: 0.657200\n",
      "Epoch  5, CIFAR-10 Batch 3:  For the train data Loss: 0.736169 Acc: 0.700000\n",
      "For the validation data Loss: 0.961112 Acc: 0.659800\n",
      "Epoch  5, CIFAR-10 Batch 4:  For the train data Loss: 0.888155 Acc: 0.725000\n",
      "For the validation data Loss: 0.946051 Acc: 0.666200\n",
      "Epoch  5, CIFAR-10 Batch 5:  For the train data Loss: 0.780280 Acc: 0.750000\n",
      "For the validation data Loss: 0.929014 Acc: 0.670400\n",
      "Epoch  6, CIFAR-10 Batch 1:  For the train data Loss: 0.916244 Acc: 0.625000\n",
      "For the validation data Loss: 0.952513 Acc: 0.670000\n",
      "Epoch  6, CIFAR-10 Batch 2:  For the train data Loss: 0.816223 Acc: 0.700000\n",
      "For the validation data Loss: 0.931200 Acc: 0.676400\n",
      "Epoch  6, CIFAR-10 Batch 3:  For the train data Loss: 0.824363 Acc: 0.675000\n",
      "For the validation data Loss: 0.988847 Acc: 0.647800\n",
      "Epoch  6, CIFAR-10 Batch 4:  For the train data Loss: 0.774517 Acc: 0.750000\n",
      "For the validation data Loss: 0.928238 Acc: 0.676200\n",
      "Epoch  6, CIFAR-10 Batch 5:  For the train data Loss: 0.702332 Acc: 0.800000\n",
      "For the validation data Loss: 0.918918 Acc: 0.680400\n",
      "Epoch  7, CIFAR-10 Batch 1:  For the train data Loss: 0.847294 Acc: 0.700000\n",
      "For the validation data Loss: 0.953278 Acc: 0.669400\n",
      "Epoch  7, CIFAR-10 Batch 2:  For the train data Loss: 0.709861 Acc: 0.700000\n",
      "For the validation data Loss: 0.904307 Acc: 0.685400\n",
      "Epoch  7, CIFAR-10 Batch 3:  For the train data Loss: 0.590189 Acc: 0.825000\n",
      "For the validation data Loss: 0.882219 Acc: 0.691200\n",
      "Epoch  7, CIFAR-10 Batch 4:  For the train data Loss: 0.759635 Acc: 0.800000\n",
      "For the validation data Loss: 0.896435 Acc: 0.686600\n",
      "Epoch  7, CIFAR-10 Batch 5:  For the train data Loss: 0.748023 Acc: 0.775000\n",
      "For the validation data Loss: 0.925326 Acc: 0.679600\n",
      "Epoch  8, CIFAR-10 Batch 1:  For the train data Loss: 0.711834 Acc: 0.725000\n",
      "For the validation data Loss: 0.932638 Acc: 0.677800\n",
      "Epoch  8, CIFAR-10 Batch 2:  For the train data Loss: 0.753086 Acc: 0.675000\n",
      "For the validation data Loss: 0.903413 Acc: 0.685600\n",
      "Epoch  8, CIFAR-10 Batch 3:  For the train data Loss: 0.630004 Acc: 0.800000\n",
      "For the validation data Loss: 0.889151 Acc: 0.692600\n",
      "Epoch  8, CIFAR-10 Batch 4:  For the train data Loss: 0.810613 Acc: 0.700000\n",
      "For the validation data Loss: 0.866922 Acc: 0.700600\n",
      "Epoch  8, CIFAR-10 Batch 5:  For the train data Loss: 0.591875 Acc: 0.775000\n",
      "For the validation data Loss: 0.897066 Acc: 0.689000\n",
      "Epoch  9, CIFAR-10 Batch 1:  For the train data Loss: 0.690310 Acc: 0.700000\n",
      "For the validation data Loss: 0.916165 Acc: 0.685600\n",
      "Epoch  9, CIFAR-10 Batch 2:  For the train data Loss: 0.602998 Acc: 0.775000\n",
      "For the validation data Loss: 0.861798 Acc: 0.706000\n",
      "Epoch  9, CIFAR-10 Batch 3:  For the train data Loss: 0.544634 Acc: 0.750000\n",
      "For the validation data Loss: 0.861476 Acc: 0.699200\n",
      "Epoch  9, CIFAR-10 Batch 4:  For the train data Loss: 0.729066 Acc: 0.825000\n",
      "For the validation data Loss: 0.860188 Acc: 0.700400\n",
      "Epoch  9, CIFAR-10 Batch 5:  For the train data Loss: 0.666019 Acc: 0.725000\n",
      "For the validation data Loss: 0.911399 Acc: 0.683400\n",
      "Epoch 10, CIFAR-10 Batch 1:  For the train data Loss: 0.715112 Acc: 0.700000\n",
      "For the validation data Loss: 0.866532 Acc: 0.703200\n",
      "Epoch 10, CIFAR-10 Batch 2:  For the train data Loss: 0.591902 Acc: 0.775000\n",
      "For the validation data Loss: 0.866273 Acc: 0.701600\n",
      "Epoch 10, CIFAR-10 Batch 3:  For the train data Loss: 0.536704 Acc: 0.775000\n",
      "For the validation data Loss: 0.868118 Acc: 0.694400\n",
      "Epoch 10, CIFAR-10 Batch 4:  For the train data Loss: 0.701613 Acc: 0.700000\n",
      "For the validation data Loss: 0.862140 Acc: 0.703000\n",
      "Epoch 10, CIFAR-10 Batch 5:  For the train data Loss: 0.516928 Acc: 0.825000\n",
      "For the validation data Loss: 0.869030 Acc: 0.696400\n",
      "Epoch 11, CIFAR-10 Batch 1:  For the train data Loss: 0.604970 Acc: 0.775000\n",
      "For the validation data Loss: 0.872433 Acc: 0.701800\n",
      "Epoch 11, CIFAR-10 Batch 2:  For the train data Loss: 0.609095 Acc: 0.725000\n",
      "For the validation data Loss: 0.842001 Acc: 0.711400\n",
      "Epoch 11, CIFAR-10 Batch 3:  For the train data Loss: 0.500171 Acc: 0.850000\n",
      "For the validation data Loss: 0.823749 Acc: 0.711200\n",
      "Epoch 11, CIFAR-10 Batch 4:  For the train data Loss: 0.657356 Acc: 0.800000\n",
      "For the validation data Loss: 0.832390 Acc: 0.716000\n",
      "Epoch 11, CIFAR-10 Batch 5:  For the train data Loss: 0.585445 Acc: 0.775000\n",
      "For the validation data Loss: 0.879015 Acc: 0.697600\n",
      "Epoch 12, CIFAR-10 Batch 1:  For the train data Loss: 0.556065 Acc: 0.825000\n",
      "For the validation data Loss: 0.890339 Acc: 0.697200\n",
      "Epoch 12, CIFAR-10 Batch 2:  For the train data Loss: 0.606905 Acc: 0.725000\n",
      "For the validation data Loss: 0.851414 Acc: 0.704000\n",
      "Epoch 12, CIFAR-10 Batch 3:  For the train data Loss: 0.454885 Acc: 0.900000\n",
      "For the validation data Loss: 0.834623 Acc: 0.713800\n",
      "Epoch 12, CIFAR-10 Batch 4:  For the train data Loss: 0.615202 Acc: 0.775000\n",
      "For the validation data Loss: 0.854465 Acc: 0.703800\n",
      "Epoch 12, CIFAR-10 Batch 5:  For the train data Loss: 0.491504 Acc: 0.825000\n",
      "For the validation data Loss: 0.868256 Acc: 0.705000\n",
      "Epoch 13, CIFAR-10 Batch 1:  For the train data Loss: 0.581974 Acc: 0.800000\n",
      "For the validation data Loss: 0.859581 Acc: 0.705600\n",
      "Epoch 13, CIFAR-10 Batch 2:  For the train data Loss: 0.535468 Acc: 0.750000\n",
      "For the validation data Loss: 0.838771 Acc: 0.704000\n",
      "Epoch 13, CIFAR-10 Batch 3:  For the train data Loss: 0.472101 Acc: 0.850000\n",
      "For the validation data Loss: 0.836784 Acc: 0.704400\n",
      "Epoch 13, CIFAR-10 Batch 4:  For the train data Loss: 0.564578 Acc: 0.825000\n",
      "For the validation data Loss: 0.831554 Acc: 0.708400\n",
      "Epoch 13, CIFAR-10 Batch 5:  For the train data Loss: 0.434392 Acc: 0.900000\n",
      "For the validation data Loss: 0.843693 Acc: 0.709200\n",
      "Epoch 14, CIFAR-10 Batch 1:  For the train data Loss: 0.539717 Acc: 0.800000\n",
      "For the validation data Loss: 0.863538 Acc: 0.704600\n",
      "Epoch 14, CIFAR-10 Batch 2:  For the train data Loss: 0.414824 Acc: 0.875000\n",
      "For the validation data Loss: 0.827813 Acc: 0.709800\n",
      "Epoch 14, CIFAR-10 Batch 3:  For the train data Loss: 0.447886 Acc: 0.875000\n",
      "For the validation data Loss: 0.819837 Acc: 0.715800\n",
      "Epoch 14, CIFAR-10 Batch 4:  For the train data Loss: 0.473859 Acc: 0.850000\n",
      "For the validation data Loss: 0.834571 Acc: 0.713000\n",
      "Epoch 14, CIFAR-10 Batch 5:  For the train data Loss: 0.536414 Acc: 0.800000\n",
      "For the validation data Loss: 0.890540 Acc: 0.696600\n",
      "Epoch 15, CIFAR-10 Batch 1:  For the train data Loss: 0.608374 Acc: 0.800000\n",
      "For the validation data Loss: 0.885193 Acc: 0.706600\n",
      "Epoch 15, CIFAR-10 Batch 2:  For the train data Loss: 0.492217 Acc: 0.825000\n",
      "For the validation data Loss: 0.825128 Acc: 0.718400\n",
      "Epoch 15, CIFAR-10 Batch 3:  For the train data Loss: 0.435293 Acc: 0.875000\n",
      "For the validation data Loss: 0.805937 Acc: 0.721000\n",
      "Epoch 15, CIFAR-10 Batch 4:  For the train data Loss: 0.535501 Acc: 0.825000\n",
      "For the validation data Loss: 0.860259 Acc: 0.705400\n",
      "Epoch 15, CIFAR-10 Batch 5:  For the train data Loss: 0.455745 Acc: 0.875000\n",
      "For the validation data Loss: 0.850143 Acc: 0.705600\n",
      "Epoch 16, CIFAR-10 Batch 1:  For the train data Loss: 0.547167 Acc: 0.825000\n",
      "For the validation data Loss: 0.840555 Acc: 0.716200\n",
      "Epoch 16, CIFAR-10 Batch 2:  For the train data Loss: 0.513464 Acc: 0.825000\n",
      "For the validation data Loss: 0.830195 Acc: 0.714200\n",
      "Epoch 16, CIFAR-10 Batch 3:  For the train data Loss: 0.365540 Acc: 0.900000\n",
      "For the validation data Loss: 0.793809 Acc: 0.730800\n",
      "Epoch 16, CIFAR-10 Batch 4:  For the train data Loss: 0.520572 Acc: 0.825000\n",
      "For the validation data Loss: 0.806916 Acc: 0.719200\n",
      "Epoch 16, CIFAR-10 Batch 5:  For the train data Loss: 0.425224 Acc: 0.825000\n",
      "For the validation data Loss: 0.828924 Acc: 0.718000\n",
      "Epoch 17, CIFAR-10 Batch 1:  For the train data Loss: 0.495604 Acc: 0.850000\n",
      "For the validation data Loss: 0.806185 Acc: 0.725400\n",
      "Epoch 17, CIFAR-10 Batch 2:  For the train data Loss: 0.530283 Acc: 0.800000\n",
      "For the validation data Loss: 0.809640 Acc: 0.721000\n",
      "Epoch 17, CIFAR-10 Batch 3:  For the train data Loss: 0.367465 Acc: 0.850000\n",
      "For the validation data Loss: 0.796897 Acc: 0.728200\n",
      "Epoch 17, CIFAR-10 Batch 4:  For the train data Loss: 0.433197 Acc: 0.850000\n",
      "For the validation data Loss: 0.824427 Acc: 0.720800\n",
      "Epoch 17, CIFAR-10 Batch 5:  For the train data Loss: 0.458012 Acc: 0.875000\n",
      "For the validation data Loss: 0.858479 Acc: 0.712600\n",
      "Epoch 18, CIFAR-10 Batch 1:  For the train data Loss: 0.561268 Acc: 0.800000\n",
      "For the validation data Loss: 0.815209 Acc: 0.719800\n",
      "Epoch 18, CIFAR-10 Batch 2:  For the train data Loss: 0.548883 Acc: 0.800000\n",
      "For the validation data Loss: 0.816159 Acc: 0.725400\n",
      "Epoch 18, CIFAR-10 Batch 3:  For the train data Loss: 0.353739 Acc: 0.925000\n",
      "For the validation data Loss: 0.834655 Acc: 0.715000\n",
      "Epoch 18, CIFAR-10 Batch 4:  For the train data Loss: 0.408539 Acc: 0.850000\n",
      "For the validation data Loss: 0.807705 Acc: 0.729000\n",
      "Epoch 18, CIFAR-10 Batch 5:  For the train data Loss: 0.437973 Acc: 0.900000\n",
      "For the validation data Loss: 0.833732 Acc: 0.719600\n",
      "Epoch 19, CIFAR-10 Batch 1:  For the train data Loss: 0.471295 Acc: 0.800000\n",
      "For the validation data Loss: 0.811326 Acc: 0.724800\n",
      "Epoch 19, CIFAR-10 Batch 2:  For the train data Loss: 0.493300 Acc: 0.825000\n",
      "For the validation data Loss: 0.814002 Acc: 0.729400\n",
      "Epoch 19, CIFAR-10 Batch 3:  For the train data Loss: 0.327377 Acc: 0.925000\n",
      "For the validation data Loss: 0.785776 Acc: 0.726800\n",
      "Epoch 19, CIFAR-10 Batch 4:  For the train data Loss: 0.395338 Acc: 0.825000\n",
      "For the validation data Loss: 0.823073 Acc: 0.720000\n",
      "Epoch 19, CIFAR-10 Batch 5:  For the train data Loss: 0.477472 Acc: 0.850000\n",
      "For the validation data Loss: 0.854148 Acc: 0.711200\n",
      "Epoch 20, CIFAR-10 Batch 1:  For the train data Loss: 0.475728 Acc: 0.825000\n",
      "For the validation data Loss: 0.821202 Acc: 0.722000\n",
      "Epoch 20, CIFAR-10 Batch 2:  For the train data Loss: 0.412156 Acc: 0.825000\n",
      "For the validation data Loss: 0.809713 Acc: 0.726800\n",
      "Epoch 20, CIFAR-10 Batch 3:  For the train data Loss: 0.418954 Acc: 0.900000\n",
      "For the validation data Loss: 0.839936 Acc: 0.720200\n",
      "Epoch 20, CIFAR-10 Batch 4:  For the train data Loss: 0.428065 Acc: 0.800000\n",
      "For the validation data Loss: 0.802842 Acc: 0.726600\n",
      "Epoch 20, CIFAR-10 Batch 5:  For the train data Loss: 0.298808 Acc: 0.925000\n",
      "For the validation data Loss: 0.817057 Acc: 0.724200\n",
      "Epoch 21, CIFAR-10 Batch 1:  For the train data Loss: 0.490971 Acc: 0.825000\n",
      "For the validation data Loss: 0.817322 Acc: 0.719800\n",
      "Epoch 21, CIFAR-10 Batch 2:  For the train data Loss: 0.396181 Acc: 0.825000\n",
      "For the validation data Loss: 0.802412 Acc: 0.723200\n",
      "Epoch 21, CIFAR-10 Batch 3:  For the train data Loss: 0.320869 Acc: 0.875000\n",
      "For the validation data Loss: 0.772668 Acc: 0.742800\n",
      "Epoch 21, CIFAR-10 Batch 4:  For the train data Loss: 0.382047 Acc: 0.850000\n",
      "For the validation data Loss: 0.807271 Acc: 0.727600\n",
      "Epoch 21, CIFAR-10 Batch 5:  For the train data Loss: 0.343874 Acc: 0.950000\n",
      "For the validation data Loss: 0.806918 Acc: 0.725400\n",
      "Epoch 22, CIFAR-10 Batch 1:  For the train data Loss: 0.488532 Acc: 0.825000\n",
      "For the validation data Loss: 0.819671 Acc: 0.726400\n",
      "Epoch 22, CIFAR-10 Batch 2:  For the train data Loss: 0.423559 Acc: 0.875000\n",
      "For the validation data Loss: 0.810796 Acc: 0.718400\n",
      "Epoch 22, CIFAR-10 Batch 3:  For the train data Loss: 0.296708 Acc: 0.850000\n",
      "For the validation data Loss: 0.816828 Acc: 0.729200\n",
      "Epoch 22, CIFAR-10 Batch 4:  For the train data Loss: 0.371674 Acc: 0.900000\n",
      "For the validation data Loss: 0.810365 Acc: 0.724400\n",
      "Epoch 22, CIFAR-10 Batch 5:  For the train data Loss: 0.325895 Acc: 0.875000\n",
      "For the validation data Loss: 0.826847 Acc: 0.717400\n",
      "Epoch 23, CIFAR-10 Batch 1:  For the train data Loss: 0.468224 Acc: 0.850000\n",
      "For the validation data Loss: 0.818360 Acc: 0.729800\n",
      "Epoch 23, CIFAR-10 Batch 2:  For the train data Loss: 0.383150 Acc: 0.925000\n",
      "For the validation data Loss: 0.802585 Acc: 0.724600\n",
      "Epoch 23, CIFAR-10 Batch 3:  For the train data Loss: 0.285026 Acc: 0.950000\n",
      "For the validation data Loss: 0.782349 Acc: 0.736400\n",
      "Epoch 23, CIFAR-10 Batch 4:  For the train data Loss: 0.474916 Acc: 0.875000\n",
      "For the validation data Loss: 0.857338 Acc: 0.707400\n",
      "Epoch 23, CIFAR-10 Batch 5:  For the train data Loss: 0.288186 Acc: 0.900000\n",
      "For the validation data Loss: 0.815161 Acc: 0.727400\n",
      "Epoch 24, CIFAR-10 Batch 1:  For the train data Loss: 0.430082 Acc: 0.825000\n",
      "For the validation data Loss: 0.795429 Acc: 0.735200\n",
      "Epoch 24, CIFAR-10 Batch 2:  For the train data Loss: 0.368264 Acc: 0.900000\n",
      "For the validation data Loss: 0.795286 Acc: 0.729600\n",
      "Epoch 24, CIFAR-10 Batch 3:  For the train data Loss: 0.317763 Acc: 0.850000\n",
      "For the validation data Loss: 0.796266 Acc: 0.729400\n",
      "Epoch 24, CIFAR-10 Batch 4:  For the train data Loss: 0.359163 Acc: 0.850000\n",
      "For the validation data Loss: 0.823992 Acc: 0.727200\n",
      "Epoch 24, CIFAR-10 Batch 5:  For the train data Loss: 0.292988 Acc: 0.925000\n",
      "For the validation data Loss: 0.836778 Acc: 0.721200\n",
      "Epoch 25, CIFAR-10 Batch 1:  For the train data Loss: 0.394780 Acc: 0.850000\n",
      "For the validation data Loss: 0.815202 Acc: 0.726200\n",
      "Epoch 25, CIFAR-10 Batch 2:  For the train data Loss: 0.442945 Acc: 0.825000\n",
      "For the validation data Loss: 0.786219 Acc: 0.737000\n",
      "Epoch 25, CIFAR-10 Batch 3:  For the train data Loss: 0.312340 Acc: 0.925000\n",
      "For the validation data Loss: 0.794159 Acc: 0.728400\n",
      "Epoch 25, CIFAR-10 Batch 4:  For the train data Loss: 0.360716 Acc: 0.900000\n",
      "For the validation data Loss: 0.836578 Acc: 0.725800\n",
      "Epoch 25, CIFAR-10 Batch 5:  For the train data Loss: 0.311792 Acc: 0.900000\n",
      "For the validation data Loss: 0.823318 Acc: 0.724800\n",
      "Epoch 26, CIFAR-10 Batch 1:  For the train data Loss: 0.473886 Acc: 0.850000\n",
      "For the validation data Loss: 0.840710 Acc: 0.719600\n",
      "Epoch 26, CIFAR-10 Batch 2:  For the train data Loss: 0.380317 Acc: 0.850000\n",
      "For the validation data Loss: 0.796266 Acc: 0.734000\n",
      "Epoch 26, CIFAR-10 Batch 3:  For the train data Loss: 0.329435 Acc: 0.875000\n",
      "For the validation data Loss: 0.822663 Acc: 0.726800\n",
      "Epoch 26, CIFAR-10 Batch 4:  For the train data Loss: 0.337926 Acc: 0.900000\n",
      "For the validation data Loss: 0.815610 Acc: 0.726600\n",
      "Epoch 26, CIFAR-10 Batch 5:  For the train data Loss: 0.259472 Acc: 0.950000\n",
      "For the validation data Loss: 0.815580 Acc: 0.728200\n",
      "Epoch 27, CIFAR-10 Batch 1:  For the train data Loss: 0.500798 Acc: 0.825000\n",
      "For the validation data Loss: 0.829327 Acc: 0.728800\n",
      "Epoch 27, CIFAR-10 Batch 2:  For the train data Loss: 0.374975 Acc: 0.850000\n",
      "For the validation data Loss: 0.814637 Acc: 0.731600\n",
      "Epoch 27, CIFAR-10 Batch 3:  For the train data Loss: 0.291670 Acc: 0.925000\n",
      "For the validation data Loss: 0.826257 Acc: 0.728000\n",
      "Epoch 27, CIFAR-10 Batch 4:  For the train data Loss: 0.278592 Acc: 0.875000\n",
      "For the validation data Loss: 0.815273 Acc: 0.728600\n",
      "Epoch 27, CIFAR-10 Batch 5:  For the train data Loss: 0.301402 Acc: 0.950000\n",
      "For the validation data Loss: 0.849520 Acc: 0.718600\n",
      "Epoch 28, CIFAR-10 Batch 1:  For the train data Loss: 0.411059 Acc: 0.850000\n",
      "For the validation data Loss: 0.839434 Acc: 0.719000\n",
      "Epoch 28, CIFAR-10 Batch 2:  For the train data Loss: 0.404073 Acc: 0.875000\n",
      "For the validation data Loss: 0.831449 Acc: 0.730800\n",
      "Epoch 28, CIFAR-10 Batch 3:  For the train data Loss: 0.257557 Acc: 0.975000\n",
      "For the validation data Loss: 0.841574 Acc: 0.728400\n",
      "Epoch 28, CIFAR-10 Batch 4:  For the train data Loss: 0.283361 Acc: 0.900000\n",
      "For the validation data Loss: 0.820750 Acc: 0.730600\n",
      "Epoch 28, CIFAR-10 Batch 5:  For the train data Loss: 0.267419 Acc: 0.950000\n",
      "For the validation data Loss: 0.863760 Acc: 0.724800\n",
      "Epoch 29, CIFAR-10 Batch 1:  For the train data Loss: 0.369867 Acc: 0.875000\n",
      "For the validation data Loss: 0.900805 Acc: 0.716000\n",
      "Epoch 29, CIFAR-10 Batch 2:  For the train data Loss: 0.367630 Acc: 0.925000\n",
      "For the validation data Loss: 0.818458 Acc: 0.729400\n",
      "Epoch 29, CIFAR-10 Batch 3:  For the train data Loss: 0.227140 Acc: 0.950000\n",
      "For the validation data Loss: 0.824074 Acc: 0.732600\n",
      "Epoch 29, CIFAR-10 Batch 4:  For the train data Loss: 0.238157 Acc: 0.925000\n",
      "For the validation data Loss: 0.827745 Acc: 0.729000\n",
      "Epoch 29, CIFAR-10 Batch 5:  For the train data Loss: 0.291475 Acc: 0.900000\n",
      "For the validation data Loss: 0.883913 Acc: 0.722400\n",
      "Epoch 30, CIFAR-10 Batch 1:  For the train data Loss: 0.353096 Acc: 0.950000\n",
      "For the validation data Loss: 0.862022 Acc: 0.724200\n",
      "Epoch 30, CIFAR-10 Batch 2:  For the train data Loss: 0.362784 Acc: 0.875000\n",
      "For the validation data Loss: 0.817397 Acc: 0.737400\n",
      "Epoch 30, CIFAR-10 Batch 3:  For the train data Loss: 0.302475 Acc: 0.950000\n",
      "For the validation data Loss: 0.810977 Acc: 0.726000\n",
      "Epoch 30, CIFAR-10 Batch 4:  For the train data Loss: 0.236417 Acc: 0.950000\n",
      "For the validation data Loss: 0.817554 Acc: 0.731200\n",
      "Epoch 30, CIFAR-10 Batch 5:  For the train data Loss: 0.280839 Acc: 0.925000\n",
      "For the validation data Loss: 0.865591 Acc: 0.723600\n",
      "Epoch 31, CIFAR-10 Batch 1:  For the train data Loss: 0.424965 Acc: 0.850000\n",
      "For the validation data Loss: 0.835377 Acc: 0.727600\n",
      "Epoch 31, CIFAR-10 Batch 2:  For the train data Loss: 0.373419 Acc: 0.875000\n",
      "For the validation data Loss: 0.821119 Acc: 0.737200\n",
      "Epoch 31, CIFAR-10 Batch 3:  For the train data Loss: 0.305097 Acc: 0.950000\n",
      "For the validation data Loss: 0.812442 Acc: 0.729800\n",
      "Epoch 31, CIFAR-10 Batch 4:  For the train data Loss: 0.264493 Acc: 0.900000\n",
      "For the validation data Loss: 0.809086 Acc: 0.731800\n",
      "Epoch 31, CIFAR-10 Batch 5:  For the train data Loss: 0.267481 Acc: 0.925000\n",
      "For the validation data Loss: 0.849430 Acc: 0.725400\n",
      "Epoch 32, CIFAR-10 Batch 1:  For the train data Loss: 0.344298 Acc: 0.875000\n",
      "For the validation data Loss: 0.813377 Acc: 0.735400\n",
      "Epoch 32, CIFAR-10 Batch 2:  For the train data Loss: 0.369605 Acc: 0.900000\n",
      "For the validation data Loss: 0.799425 Acc: 0.738000\n",
      "Epoch 32, CIFAR-10 Batch 3:  For the train data Loss: 0.215475 Acc: 0.950000\n",
      "For the validation data Loss: 0.837896 Acc: 0.729000\n",
      "Epoch 32, CIFAR-10 Batch 4:  For the train data Loss: 0.292261 Acc: 0.900000\n",
      "For the validation data Loss: 0.860099 Acc: 0.729600\n",
      "Epoch 32, CIFAR-10 Batch 5:  For the train data Loss: 0.316503 Acc: 0.875000\n",
      "For the validation data Loss: 0.914894 Acc: 0.720800\n",
      "Epoch 33, CIFAR-10 Batch 1:  For the train data Loss: 0.344721 Acc: 0.900000\n",
      "For the validation data Loss: 0.866369 Acc: 0.718200\n",
      "Epoch 33, CIFAR-10 Batch 2:  For the train data Loss: 0.314438 Acc: 0.900000\n",
      "For the validation data Loss: 0.807295 Acc: 0.743000\n",
      "Epoch 33, CIFAR-10 Batch 3:  For the train data Loss: 0.212416 Acc: 0.950000\n",
      "For the validation data Loss: 0.848088 Acc: 0.723000\n",
      "Epoch 33, CIFAR-10 Batch 4:  For the train data Loss: 0.212530 Acc: 0.950000\n",
      "For the validation data Loss: 0.823064 Acc: 0.740800\n",
      "Epoch 33, CIFAR-10 Batch 5:  For the train data Loss: 0.272727 Acc: 0.925000\n",
      "For the validation data Loss: 0.863455 Acc: 0.726600\n",
      "Epoch 34, CIFAR-10 Batch 1:  For the train data Loss: 0.309212 Acc: 0.875000\n",
      "For the validation data Loss: 0.846148 Acc: 0.731600\n",
      "Epoch 34, CIFAR-10 Batch 2:  For the train data Loss: 0.353714 Acc: 0.900000\n",
      "For the validation data Loss: 0.839842 Acc: 0.732000\n",
      "Epoch 34, CIFAR-10 Batch 3:  For the train data Loss: 0.240240 Acc: 0.925000\n",
      "For the validation data Loss: 0.831302 Acc: 0.735600\n",
      "Epoch 34, CIFAR-10 Batch 4:  For the train data Loss: 0.201957 Acc: 0.950000\n",
      "For the validation data Loss: 0.869274 Acc: 0.729600\n",
      "Epoch 34, CIFAR-10 Batch 5:  For the train data Loss: 0.229452 Acc: 0.975000\n",
      "For the validation data Loss: 0.860092 Acc: 0.724000\n",
      "Epoch 35, CIFAR-10 Batch 1:  For the train data Loss: 0.336201 Acc: 0.900000\n",
      "For the validation data Loss: 0.869968 Acc: 0.730200\n",
      "Epoch 35, CIFAR-10 Batch 2:  For the train data Loss: 0.285518 Acc: 0.925000\n",
      "For the validation data Loss: 0.821712 Acc: 0.732600\n",
      "Epoch 35, CIFAR-10 Batch 3:  For the train data Loss: 0.234441 Acc: 0.950000\n",
      "For the validation data Loss: 0.871577 Acc: 0.719000\n",
      "Epoch 35, CIFAR-10 Batch 4:  For the train data Loss: 0.246863 Acc: 0.925000\n",
      "For the validation data Loss: 0.855197 Acc: 0.729400\n",
      "Epoch 35, CIFAR-10 Batch 5:  For the train data Loss: 0.248175 Acc: 0.925000\n",
      "For the validation data Loss: 0.853985 Acc: 0.721600\n",
      "Epoch 36, CIFAR-10 Batch 1:  For the train data Loss: 0.341545 Acc: 0.850000\n",
      "For the validation data Loss: 0.860788 Acc: 0.731400\n",
      "Epoch 36, CIFAR-10 Batch 2:  For the train data Loss: 0.331516 Acc: 0.900000\n",
      "For the validation data Loss: 0.861405 Acc: 0.733200\n",
      "Epoch 36, CIFAR-10 Batch 3:  For the train data Loss: 0.237933 Acc: 0.925000\n",
      "For the validation data Loss: 0.850684 Acc: 0.732600\n",
      "Epoch 36, CIFAR-10 Batch 4:  For the train data Loss: 0.251094 Acc: 0.900000\n",
      "For the validation data Loss: 0.879517 Acc: 0.726800\n",
      "Epoch 36, CIFAR-10 Batch 5:  For the train data Loss: 0.189556 Acc: 0.975000\n",
      "For the validation data Loss: 0.846817 Acc: 0.731200\n",
      "Epoch 37, CIFAR-10 Batch 1:  For the train data Loss: 0.333646 Acc: 0.850000\n",
      "For the validation data Loss: 0.862054 Acc: 0.728800\n",
      "Epoch 37, CIFAR-10 Batch 2:  For the train data Loss: 0.242908 Acc: 0.950000\n",
      "For the validation data Loss: 0.813866 Acc: 0.743400\n",
      "Epoch 37, CIFAR-10 Batch 3:  For the train data Loss: 0.249919 Acc: 0.950000\n",
      "For the validation data Loss: 0.920323 Acc: 0.709200\n",
      "Epoch 37, CIFAR-10 Batch 4:  For the train data Loss: 0.231414 Acc: 0.950000\n",
      "For the validation data Loss: 0.841573 Acc: 0.732000\n",
      "Epoch 37, CIFAR-10 Batch 5:  For the train data Loss: 0.169329 Acc: 1.000000\n",
      "For the validation data Loss: 0.866041 Acc: 0.729600\n",
      "Epoch 38, CIFAR-10 Batch 1:  For the train data Loss: 0.395856 Acc: 0.825000\n",
      "For the validation data Loss: 0.889807 Acc: 0.727000\n",
      "Epoch 38, CIFAR-10 Batch 2:  For the train data Loss: 0.226995 Acc: 0.975000\n",
      "For the validation data Loss: 0.824789 Acc: 0.732600\n",
      "Epoch 38, CIFAR-10 Batch 3:  For the train data Loss: 0.222656 Acc: 0.950000\n",
      "For the validation data Loss: 0.866845 Acc: 0.721200\n",
      "Epoch 38, CIFAR-10 Batch 4:  For the train data Loss: 0.205422 Acc: 0.950000\n",
      "For the validation data Loss: 0.826544 Acc: 0.736000\n",
      "Epoch 38, CIFAR-10 Batch 5:  For the train data Loss: 0.197241 Acc: 0.975000\n",
      "For the validation data Loss: 0.870660 Acc: 0.728800\n",
      "Epoch 39, CIFAR-10 Batch 1:  For the train data Loss: 0.365210 Acc: 0.850000\n",
      "For the validation data Loss: 0.900334 Acc: 0.731200\n",
      "Epoch 39, CIFAR-10 Batch 2:  For the train data Loss: 0.266789 Acc: 0.925000\n",
      "For the validation data Loss: 0.847345 Acc: 0.737600\n",
      "Epoch 39, CIFAR-10 Batch 3:  For the train data Loss: 0.240680 Acc: 0.925000\n",
      "For the validation data Loss: 0.889342 Acc: 0.729200\n",
      "Epoch 39, CIFAR-10 Batch 4:  For the train data Loss: 0.241069 Acc: 0.925000\n",
      "For the validation data Loss: 0.867646 Acc: 0.732400\n",
      "Epoch 39, CIFAR-10 Batch 5:  For the train data Loss: 0.208834 Acc: 0.950000\n",
      "For the validation data Loss: 0.863137 Acc: 0.730400\n",
      "Epoch 40, CIFAR-10 Batch 1:  For the train data Loss: 0.358681 Acc: 0.825000\n",
      "For the validation data Loss: 0.894713 Acc: 0.731600\n",
      "Epoch 40, CIFAR-10 Batch 2:  For the train data Loss: 0.230402 Acc: 0.950000\n",
      "For the validation data Loss: 0.868172 Acc: 0.734800\n",
      "Epoch 40, CIFAR-10 Batch 3:  For the train data Loss: 0.182218 Acc: 0.975000\n",
      "For the validation data Loss: 0.876442 Acc: 0.729200\n",
      "Epoch 40, CIFAR-10 Batch 4:  For the train data Loss: 0.208317 Acc: 0.925000\n",
      "For the validation data Loss: 0.854357 Acc: 0.736000\n",
      "Epoch 40, CIFAR-10 Batch 5:  For the train data Loss: 0.207447 Acc: 0.950000\n",
      "For the validation data Loss: 0.842306 Acc: 0.733800\n",
      "Epoch 41, CIFAR-10 Batch 1:  For the train data Loss: 0.318514 Acc: 0.825000\n",
      "For the validation data Loss: 0.870289 Acc: 0.739200\n",
      "Epoch 41, CIFAR-10 Batch 2:  For the train data Loss: 0.292611 Acc: 0.950000\n",
      "For the validation data Loss: 0.854934 Acc: 0.727200\n",
      "Epoch 41, CIFAR-10 Batch 3:  For the train data Loss: 0.212860 Acc: 0.975000\n",
      "For the validation data Loss: 0.903672 Acc: 0.712600\n",
      "Epoch 41, CIFAR-10 Batch 4:  For the train data Loss: 0.157097 Acc: 0.975000\n",
      "For the validation data Loss: 0.850285 Acc: 0.738400\n",
      "Epoch 41, CIFAR-10 Batch 5:  For the train data Loss: 0.171021 Acc: 0.950000\n",
      "For the validation data Loss: 0.883317 Acc: 0.727600\n",
      "Epoch 42, CIFAR-10 Batch 1:  For the train data Loss: 0.309330 Acc: 0.875000\n",
      "For the validation data Loss: 0.860285 Acc: 0.739800\n",
      "Epoch 42, CIFAR-10 Batch 2:  For the train data Loss: 0.214916 Acc: 0.975000\n",
      "For the validation data Loss: 0.886351 Acc: 0.731600\n",
      "Epoch 42, CIFAR-10 Batch 3:  For the train data Loss: 0.169229 Acc: 0.975000\n",
      "For the validation data Loss: 0.875191 Acc: 0.728400\n",
      "Epoch 42, CIFAR-10 Batch 4:  For the train data Loss: 0.207830 Acc: 0.925000\n",
      "For the validation data Loss: 0.867028 Acc: 0.738600\n",
      "Epoch 42, CIFAR-10 Batch 5:  For the train data Loss: 0.183063 Acc: 0.925000\n",
      "For the validation data Loss: 0.867088 Acc: 0.741800\n",
      "Epoch 43, CIFAR-10 Batch 1:  For the train data Loss: 0.339027 Acc: 0.850000\n",
      "For the validation data Loss: 0.872896 Acc: 0.734400\n",
      "Epoch 43, CIFAR-10 Batch 2:  For the train data Loss: 0.192006 Acc: 0.975000\n",
      "For the validation data Loss: 0.860176 Acc: 0.744200\n",
      "Epoch 43, CIFAR-10 Batch 3:  For the train data Loss: 0.239368 Acc: 0.950000\n",
      "For the validation data Loss: 0.903954 Acc: 0.727000\n",
      "Epoch 43, CIFAR-10 Batch 4:  For the train data Loss: 0.218237 Acc: 0.975000\n",
      "For the validation data Loss: 0.842700 Acc: 0.727800\n",
      "Epoch 43, CIFAR-10 Batch 5:  For the train data Loss: 0.116112 Acc: 1.000000\n",
      "For the validation data Loss: 0.874125 Acc: 0.729000\n",
      "Epoch 44, CIFAR-10 Batch 1:  For the train data Loss: 0.331072 Acc: 0.875000\n",
      "For the validation data Loss: 0.859207 Acc: 0.738200\n",
      "Epoch 44, CIFAR-10 Batch 2:  For the train data Loss: 0.247297 Acc: 0.900000\n",
      "For the validation data Loss: 0.870305 Acc: 0.731600\n",
      "Epoch 44, CIFAR-10 Batch 3:  For the train data Loss: 0.227382 Acc: 0.975000\n",
      "For the validation data Loss: 0.872392 Acc: 0.725000\n",
      "Epoch 44, CIFAR-10 Batch 4:  For the train data Loss: 0.186515 Acc: 0.950000\n",
      "For the validation data Loss: 0.897338 Acc: 0.727600\n",
      "Epoch 44, CIFAR-10 Batch 5:  For the train data Loss: 0.184420 Acc: 0.950000\n",
      "For the validation data Loss: 0.892972 Acc: 0.739600\n",
      "Epoch 45, CIFAR-10 Batch 1:  For the train data Loss: 0.252710 Acc: 0.950000\n",
      "For the validation data Loss: 0.891220 Acc: 0.729200\n",
      "Epoch 45, CIFAR-10 Batch 2:  For the train data Loss: 0.202061 Acc: 0.950000\n",
      "For the validation data Loss: 0.861707 Acc: 0.733400\n",
      "Epoch 45, CIFAR-10 Batch 3:  For the train data Loss: 0.194307 Acc: 0.925000\n",
      "For the validation data Loss: 0.882083 Acc: 0.724400\n",
      "Epoch 45, CIFAR-10 Batch 4:  For the train data Loss: 0.167128 Acc: 0.950000\n",
      "For the validation data Loss: 0.858255 Acc: 0.744800\n",
      "Epoch 45, CIFAR-10 Batch 5:  For the train data Loss: 0.165211 Acc: 0.975000\n",
      "For the validation data Loss: 0.908760 Acc: 0.734200\n",
      "Epoch 46, CIFAR-10 Batch 1:  For the train data Loss: 0.341564 Acc: 0.875000\n",
      "For the validation data Loss: 0.910935 Acc: 0.722400\n",
      "Epoch 46, CIFAR-10 Batch 2:  For the train data Loss: 0.193258 Acc: 0.975000\n",
      "For the validation data Loss: 0.887722 Acc: 0.731600\n",
      "Epoch 46, CIFAR-10 Batch 3:  For the train data Loss: 0.241710 Acc: 0.925000\n",
      "For the validation data Loss: 0.960009 Acc: 0.713200\n",
      "Epoch 46, CIFAR-10 Batch 4:  For the train data Loss: 0.194697 Acc: 0.925000\n",
      "For the validation data Loss: 0.893481 Acc: 0.736000\n",
      "Epoch 46, CIFAR-10 Batch 5:  For the train data Loss: 0.166907 Acc: 0.950000\n",
      "For the validation data Loss: 0.879329 Acc: 0.738600\n",
      "Epoch 47, CIFAR-10 Batch 1:  For the train data Loss: 0.255501 Acc: 0.900000\n",
      "For the validation data Loss: 0.911366 Acc: 0.735800\n",
      "Epoch 47, CIFAR-10 Batch 2:  For the train data Loss: 0.185017 Acc: 0.950000\n",
      "For the validation data Loss: 0.882393 Acc: 0.733600\n",
      "Epoch 47, CIFAR-10 Batch 3:  For the train data Loss: 0.157645 Acc: 0.975000\n",
      "For the validation data Loss: 0.892047 Acc: 0.734000\n",
      "Epoch 47, CIFAR-10 Batch 4:  For the train data Loss: 0.177452 Acc: 0.975000\n",
      "For the validation data Loss: 0.876793 Acc: 0.733400\n",
      "Epoch 47, CIFAR-10 Batch 5:  For the train data Loss: 0.173344 Acc: 0.975000\n",
      "For the validation data Loss: 0.922234 Acc: 0.719000\n",
      "Epoch 48, CIFAR-10 Batch 1:  For the train data Loss: 0.239722 Acc: 0.925000\n",
      "For the validation data Loss: 0.901136 Acc: 0.739400\n",
      "Epoch 48, CIFAR-10 Batch 2:  For the train data Loss: 0.236964 Acc: 0.900000\n",
      "For the validation data Loss: 0.848887 Acc: 0.740000\n",
      "Epoch 48, CIFAR-10 Batch 3:  For the train data Loss: 0.122587 Acc: 0.975000\n",
      "For the validation data Loss: 0.898263 Acc: 0.729400\n",
      "Epoch 48, CIFAR-10 Batch 4:  For the train data Loss: 0.139216 Acc: 0.975000\n",
      "For the validation data Loss: 0.893893 Acc: 0.741200\n",
      "Epoch 48, CIFAR-10 Batch 5:  For the train data Loss: 0.124382 Acc: 0.975000\n",
      "For the validation data Loss: 0.943319 Acc: 0.724800\n",
      "Epoch 49, CIFAR-10 Batch 1:  For the train data Loss: 0.270511 Acc: 0.925000\n",
      "For the validation data Loss: 0.911072 Acc: 0.731200\n",
      "Epoch 49, CIFAR-10 Batch 2:  For the train data Loss: 0.217617 Acc: 0.900000\n",
      "For the validation data Loss: 0.898927 Acc: 0.728000\n",
      "Epoch 49, CIFAR-10 Batch 3:  For the train data Loss: 0.169215 Acc: 0.950000\n",
      "For the validation data Loss: 0.886513 Acc: 0.736600\n",
      "Epoch 49, CIFAR-10 Batch 4:  For the train data Loss: 0.145181 Acc: 1.000000\n",
      "For the validation data Loss: 0.895754 Acc: 0.739600\n",
      "Epoch 49, CIFAR-10 Batch 5:  For the train data Loss: 0.161648 Acc: 0.950000\n",
      "For the validation data Loss: 0.917446 Acc: 0.730000\n",
      "Epoch 50, CIFAR-10 Batch 1:  For the train data Loss: 0.221041 Acc: 0.925000\n",
      "For the validation data Loss: 0.923374 Acc: 0.738400\n",
      "Epoch 50, CIFAR-10 Batch 2:  For the train data Loss: 0.204841 Acc: 0.975000\n",
      "For the validation data Loss: 0.920073 Acc: 0.734200\n",
      "Epoch 50, CIFAR-10 Batch 3:  For the train data Loss: 0.172526 Acc: 1.000000\n",
      "For the validation data Loss: 0.879271 Acc: 0.732800\n",
      "Epoch 50, CIFAR-10 Batch 4:  For the train data Loss: 0.213539 Acc: 0.900000\n",
      "For the validation data Loss: 0.942413 Acc: 0.724400\n",
      "Epoch 50, CIFAR-10 Batch 5:  For the train data Loss: 0.122027 Acc: 1.000000\n",
      "For the validation data Loss: 0.888656 Acc: 0.733600\n",
      "Epoch 51, CIFAR-10 Batch 1:  For the train data Loss: 0.279370 Acc: 0.875000\n",
      "For the validation data Loss: 0.918792 Acc: 0.731600\n",
      "Epoch 51, CIFAR-10 Batch 2:  For the train data Loss: 0.152729 Acc: 0.975000\n",
      "For the validation data Loss: 0.868600 Acc: 0.739600\n",
      "Epoch 51, CIFAR-10 Batch 3:  For the train data Loss: 0.157597 Acc: 0.975000\n",
      "For the validation data Loss: 0.953583 Acc: 0.718800\n",
      "Epoch 51, CIFAR-10 Batch 4:  For the train data Loss: 0.140022 Acc: 0.975000\n",
      "For the validation data Loss: 0.873502 Acc: 0.735200\n",
      "Epoch 51, CIFAR-10 Batch 5:  For the train data Loss: 0.136183 Acc: 0.975000\n",
      "For the validation data Loss: 0.907411 Acc: 0.725600\n",
      "Epoch 52, CIFAR-10 Batch 1:  For the train data Loss: 0.211728 Acc: 0.975000\n",
      "For the validation data Loss: 0.900068 Acc: 0.736200\n",
      "Epoch 52, CIFAR-10 Batch 2:  For the train data Loss: 0.246336 Acc: 0.900000\n",
      "For the validation data Loss: 0.902834 Acc: 0.732200\n",
      "Epoch 52, CIFAR-10 Batch 3:  For the train data Loss: 0.188913 Acc: 0.950000\n",
      "For the validation data Loss: 0.912708 Acc: 0.731000\n",
      "Epoch 52, CIFAR-10 Batch 4:  For the train data Loss: 0.185167 Acc: 0.925000\n",
      "For the validation data Loss: 0.919694 Acc: 0.730200\n",
      "Epoch 52, CIFAR-10 Batch 5:  For the train data Loss: 0.089854 Acc: 1.000000\n",
      "For the validation data Loss: 0.891645 Acc: 0.734000\n",
      "Epoch 53, CIFAR-10 Batch 1:  For the train data Loss: 0.282785 Acc: 0.900000\n",
      "For the validation data Loss: 0.884541 Acc: 0.734400\n",
      "Epoch 53, CIFAR-10 Batch 2:  For the train data Loss: 0.240100 Acc: 0.925000\n",
      "For the validation data Loss: 0.931372 Acc: 0.731600\n",
      "Epoch 53, CIFAR-10 Batch 3:  For the train data Loss: 0.183002 Acc: 0.950000\n",
      "For the validation data Loss: 0.950309 Acc: 0.716600\n",
      "Epoch 53, CIFAR-10 Batch 4:  For the train data Loss: 0.141979 Acc: 1.000000\n",
      "For the validation data Loss: 0.939601 Acc: 0.739400\n",
      "Epoch 53, CIFAR-10 Batch 5:  For the train data Loss: 0.138418 Acc: 1.000000\n",
      "For the validation data Loss: 0.911538 Acc: 0.735400\n",
      "Epoch 54, CIFAR-10 Batch 1:  For the train data Loss: 0.263594 Acc: 0.925000\n",
      "For the validation data Loss: 0.958474 Acc: 0.735000\n",
      "Epoch 54, CIFAR-10 Batch 2:  For the train data Loss: 0.286312 Acc: 0.925000\n",
      "For the validation data Loss: 0.942919 Acc: 0.731200\n",
      "Epoch 54, CIFAR-10 Batch 3:  For the train data Loss: 0.121436 Acc: 1.000000\n",
      "For the validation data Loss: 0.920883 Acc: 0.723800\n",
      "Epoch 54, CIFAR-10 Batch 4:  For the train data Loss: 0.162633 Acc: 0.975000\n",
      "For the validation data Loss: 0.893549 Acc: 0.735000\n",
      "Epoch 54, CIFAR-10 Batch 5:  For the train data Loss: 0.211940 Acc: 0.950000\n",
      "For the validation data Loss: 0.914840 Acc: 0.729600\n",
      "Epoch 55, CIFAR-10 Batch 1:  For the train data Loss: 0.217630 Acc: 0.950000\n",
      "For the validation data Loss: 0.936326 Acc: 0.737000\n",
      "Epoch 55, CIFAR-10 Batch 2:  For the train data Loss: 0.184353 Acc: 0.950000\n",
      "For the validation data Loss: 0.934567 Acc: 0.732000\n",
      "Epoch 55, CIFAR-10 Batch 3:  For the train data Loss: 0.135821 Acc: 1.000000\n",
      "For the validation data Loss: 0.954932 Acc: 0.729200\n",
      "Epoch 55, CIFAR-10 Batch 4:  For the train data Loss: 0.175646 Acc: 0.975000\n",
      "For the validation data Loss: 0.932412 Acc: 0.728400\n",
      "Epoch 55, CIFAR-10 Batch 5:  For the train data Loss: 0.164763 Acc: 0.950000\n",
      "For the validation data Loss: 0.915822 Acc: 0.740600\n",
      "Epoch 56, CIFAR-10 Batch 1:  For the train data Loss: 0.284538 Acc: 0.900000\n",
      "For the validation data Loss: 0.920930 Acc: 0.738600\n",
      "Epoch 56, CIFAR-10 Batch 2:  For the train data Loss: 0.138609 Acc: 0.950000\n",
      "For the validation data Loss: 0.935992 Acc: 0.732200\n",
      "Epoch 56, CIFAR-10 Batch 3:  For the train data Loss: 0.122246 Acc: 0.950000\n",
      "For the validation data Loss: 0.990701 Acc: 0.723400\n",
      "Epoch 56, CIFAR-10 Batch 4:  For the train data Loss: 0.125176 Acc: 0.975000\n",
      "For the validation data Loss: 0.935688 Acc: 0.721800\n",
      "Epoch 56, CIFAR-10 Batch 5:  For the train data Loss: 0.130092 Acc: 0.975000\n",
      "For the validation data Loss: 0.896372 Acc: 0.741800\n",
      "Epoch 57, CIFAR-10 Batch 1:  For the train data Loss: 0.255938 Acc: 0.950000\n",
      "For the validation data Loss: 0.935628 Acc: 0.742800\n",
      "Epoch 57, CIFAR-10 Batch 2:  For the train data Loss: 0.182341 Acc: 0.950000\n",
      "For the validation data Loss: 0.926200 Acc: 0.734200\n",
      "Epoch 57, CIFAR-10 Batch 3:  For the train data Loss: 0.169622 Acc: 0.975000\n",
      "For the validation data Loss: 0.947824 Acc: 0.722400\n",
      "Epoch 57, CIFAR-10 Batch 4:  For the train data Loss: 0.151289 Acc: 0.975000\n",
      "For the validation data Loss: 0.938856 Acc: 0.734800\n",
      "Epoch 57, CIFAR-10 Batch 5:  For the train data Loss: 0.146958 Acc: 0.975000\n",
      "For the validation data Loss: 0.950095 Acc: 0.735400\n",
      "Epoch 58, CIFAR-10 Batch 1:  For the train data Loss: 0.239636 Acc: 0.925000\n",
      "For the validation data Loss: 0.941685 Acc: 0.734200\n",
      "Epoch 58, CIFAR-10 Batch 2:  For the train data Loss: 0.208263 Acc: 0.950000\n",
      "For the validation data Loss: 0.978509 Acc: 0.719200\n",
      "Epoch 58, CIFAR-10 Batch 3:  For the train data Loss: 0.106571 Acc: 0.975000\n",
      "For the validation data Loss: 0.943908 Acc: 0.722400\n",
      "Epoch 58, CIFAR-10 Batch 4:  For the train data Loss: 0.151180 Acc: 0.925000\n",
      "For the validation data Loss: 0.953799 Acc: 0.739400\n",
      "Epoch 58, CIFAR-10 Batch 5:  For the train data Loss: 0.154408 Acc: 0.975000\n",
      "For the validation data Loss: 0.989781 Acc: 0.727200\n",
      "Epoch 59, CIFAR-10 Batch 1:  For the train data Loss: 0.199041 Acc: 0.975000\n",
      "For the validation data Loss: 0.960155 Acc: 0.724000\n",
      "Epoch 59, CIFAR-10 Batch 2:  For the train data Loss: 0.197409 Acc: 0.900000\n",
      "For the validation data Loss: 0.912375 Acc: 0.740200\n",
      "Epoch 59, CIFAR-10 Batch 3:  For the train data Loss: 0.109404 Acc: 0.975000\n",
      "For the validation data Loss: 0.931650 Acc: 0.738600\n",
      "Epoch 59, CIFAR-10 Batch 4:  For the train data Loss: 0.136664 Acc: 0.950000\n",
      "For the validation data Loss: 0.925724 Acc: 0.727200\n",
      "Epoch 59, CIFAR-10 Batch 5:  For the train data Loss: 0.239252 Acc: 0.900000\n",
      "For the validation data Loss: 0.940197 Acc: 0.738000\n",
      "Epoch 60, CIFAR-10 Batch 1:  For the train data Loss: 0.208942 Acc: 0.975000\n",
      "For the validation data Loss: 0.948458 Acc: 0.733800\n",
      "Epoch 60, CIFAR-10 Batch 2:  For the train data Loss: 0.191684 Acc: 0.975000\n",
      "For the validation data Loss: 0.908382 Acc: 0.742400\n",
      "Epoch 60, CIFAR-10 Batch 3:  For the train data Loss: 0.124017 Acc: 0.975000\n",
      "For the validation data Loss: 0.955507 Acc: 0.728000\n",
      "Epoch 60, CIFAR-10 Batch 4:  For the train data Loss: 0.103764 Acc: 0.975000\n",
      "For the validation data Loss: 0.939780 Acc: 0.734400\n",
      "Epoch 60, CIFAR-10 Batch 5:  For the train data Loss: 0.202039 Acc: 0.925000\n",
      "For the validation data Loss: 0.974325 Acc: 0.724800\n",
      "Epoch 61, CIFAR-10 Batch 1:  For the train data Loss: 0.162924 Acc: 0.975000\n",
      "For the validation data Loss: 0.968231 Acc: 0.728800\n",
      "Epoch 61, CIFAR-10 Batch 2:  For the train data Loss: 0.161968 Acc: 0.975000\n",
      "For the validation data Loss: 0.949291 Acc: 0.730200\n",
      "Epoch 61, CIFAR-10 Batch 3:  For the train data Loss: 0.113909 Acc: 0.975000\n",
      "For the validation data Loss: 0.934889 Acc: 0.726200\n",
      "Epoch 61, CIFAR-10 Batch 4:  For the train data Loss: 0.126039 Acc: 0.975000\n",
      "For the validation data Loss: 0.944715 Acc: 0.730400\n",
      "Epoch 61, CIFAR-10 Batch 5:  For the train data Loss: 0.096224 Acc: 1.000000\n",
      "For the validation data Loss: 0.937486 Acc: 0.736800\n",
      "Epoch 62, CIFAR-10 Batch 1:  For the train data Loss: 0.183115 Acc: 0.975000\n",
      "For the validation data Loss: 0.963514 Acc: 0.733000\n",
      "Epoch 62, CIFAR-10 Batch 2:  For the train data Loss: 0.141886 Acc: 0.975000\n",
      "For the validation data Loss: 0.954305 Acc: 0.725600\n",
      "Epoch 62, CIFAR-10 Batch 3:  For the train data Loss: 0.122996 Acc: 1.000000\n",
      "For the validation data Loss: 1.026838 Acc: 0.713200\n",
      "Epoch 62, CIFAR-10 Batch 4:  For the train data Loss: 0.152063 Acc: 0.950000\n",
      "For the validation data Loss: 0.938740 Acc: 0.740800\n",
      "Epoch 62, CIFAR-10 Batch 5:  For the train data Loss: 0.134548 Acc: 0.975000\n",
      "For the validation data Loss: 0.951686 Acc: 0.731600\n",
      "Epoch 63, CIFAR-10 Batch 1:  For the train data Loss: 0.180389 Acc: 1.000000\n",
      "For the validation data Loss: 0.986799 Acc: 0.738400\n",
      "Epoch 63, CIFAR-10 Batch 2:  For the train data Loss: 0.119374 Acc: 1.000000\n",
      "For the validation data Loss: 0.937465 Acc: 0.737200\n",
      "Epoch 63, CIFAR-10 Batch 3:  For the train data Loss: 0.114335 Acc: 0.975000\n",
      "For the validation data Loss: 0.978487 Acc: 0.730600\n",
      "Epoch 63, CIFAR-10 Batch 4:  For the train data Loss: 0.103337 Acc: 1.000000\n",
      "For the validation data Loss: 0.976310 Acc: 0.735200\n",
      "Epoch 63, CIFAR-10 Batch 5:  For the train data Loss: 0.134457 Acc: 1.000000\n",
      "For the validation data Loss: 0.958271 Acc: 0.737400\n",
      "Epoch 64, CIFAR-10 Batch 1:  For the train data Loss: 0.198336 Acc: 0.975000\n",
      "For the validation data Loss: 0.943834 Acc: 0.738400\n",
      "Epoch 64, CIFAR-10 Batch 2:  For the train data Loss: 0.151931 Acc: 1.000000\n",
      "For the validation data Loss: 0.973655 Acc: 0.741600\n",
      "Epoch 64, CIFAR-10 Batch 3:  For the train data Loss: 0.137373 Acc: 0.925000\n",
      "For the validation data Loss: 0.981614 Acc: 0.718400\n",
      "Epoch 64, CIFAR-10 Batch 4:  For the train data Loss: 0.105153 Acc: 0.975000\n",
      "For the validation data Loss: 0.983572 Acc: 0.732800\n",
      "Epoch 64, CIFAR-10 Batch 5:  For the train data Loss: 0.096187 Acc: 1.000000\n",
      "For the validation data Loss: 0.939960 Acc: 0.734000\n",
      "Epoch 65, CIFAR-10 Batch 1:  For the train data Loss: 0.202354 Acc: 0.950000\n",
      "For the validation data Loss: 0.983064 Acc: 0.729800\n",
      "Epoch 65, CIFAR-10 Batch 2:  For the train data Loss: 0.212243 Acc: 0.950000\n",
      "For the validation data Loss: 0.951400 Acc: 0.734800\n",
      "Epoch 65, CIFAR-10 Batch 3:  For the train data Loss: 0.120552 Acc: 1.000000\n",
      "For the validation data Loss: 0.996857 Acc: 0.724600\n",
      "Epoch 65, CIFAR-10 Batch 4:  For the train data Loss: 0.092327 Acc: 0.975000\n",
      "For the validation data Loss: 1.000360 Acc: 0.727000\n",
      "Epoch 65, CIFAR-10 Batch 5:  For the train data Loss: 0.091315 Acc: 0.975000\n",
      "For the validation data Loss: 0.981092 Acc: 0.736200\n",
      "Epoch 66, CIFAR-10 Batch 1:  For the train data Loss: 0.232537 Acc: 0.925000\n",
      "For the validation data Loss: 0.994001 Acc: 0.736600\n",
      "Epoch 66, CIFAR-10 Batch 2:  For the train data Loss: 0.121231 Acc: 0.975000\n",
      "For the validation data Loss: 0.966248 Acc: 0.729200\n",
      "Epoch 66, CIFAR-10 Batch 3:  For the train data Loss: 0.127409 Acc: 0.950000\n",
      "For the validation data Loss: 1.012856 Acc: 0.717400\n",
      "Epoch 66, CIFAR-10 Batch 4:  For the train data Loss: 0.174531 Acc: 0.950000\n",
      "For the validation data Loss: 0.968327 Acc: 0.727800\n",
      "Epoch 66, CIFAR-10 Batch 5:  For the train data Loss: 0.100877 Acc: 0.975000\n",
      "For the validation data Loss: 0.982819 Acc: 0.734400\n",
      "Epoch 67, CIFAR-10 Batch 1:  For the train data Loss: 0.254571 Acc: 0.925000\n",
      "For the validation data Loss: 1.020049 Acc: 0.731000\n",
      "Epoch 67, CIFAR-10 Batch 2:  For the train data Loss: 0.117648 Acc: 1.000000\n",
      "For the validation data Loss: 0.974985 Acc: 0.731000\n",
      "Epoch 67, CIFAR-10 Batch 3:  For the train data Loss: 0.122269 Acc: 0.975000\n",
      "For the validation data Loss: 0.951195 Acc: 0.736600\n",
      "Epoch 67, CIFAR-10 Batch 4:  For the train data Loss: 0.181233 Acc: 0.900000\n",
      "For the validation data Loss: 0.948819 Acc: 0.733600\n",
      "Epoch 67, CIFAR-10 Batch 5:  For the train data Loss: 0.084595 Acc: 1.000000\n",
      "For the validation data Loss: 0.986845 Acc: 0.734600\n",
      "Epoch 68, CIFAR-10 Batch 1:  For the train data Loss: 0.159425 Acc: 0.925000\n",
      "For the validation data Loss: 1.029430 Acc: 0.735000\n",
      "Epoch 68, CIFAR-10 Batch 2:  For the train data Loss: 0.088796 Acc: 1.000000\n",
      "For the validation data Loss: 0.950311 Acc: 0.738600\n",
      "Epoch 68, CIFAR-10 Batch 3:  For the train data Loss: 0.124043 Acc: 0.975000\n",
      "For the validation data Loss: 0.964263 Acc: 0.733400\n",
      "Epoch 68, CIFAR-10 Batch 4:  For the train data Loss: 0.107014 Acc: 1.000000\n",
      "For the validation data Loss: 0.974233 Acc: 0.732000\n",
      "Epoch 68, CIFAR-10 Batch 5:  For the train data Loss: 0.115117 Acc: 1.000000\n",
      "For the validation data Loss: 0.955707 Acc: 0.738800\n",
      "Epoch 69, CIFAR-10 Batch 1:  For the train data Loss: 0.176077 Acc: 0.950000\n",
      "For the validation data Loss: 0.965646 Acc: 0.739800\n",
      "Epoch 69, CIFAR-10 Batch 2:  For the train data Loss: 0.090538 Acc: 1.000000\n",
      "For the validation data Loss: 0.954915 Acc: 0.738600\n",
      "Epoch 69, CIFAR-10 Batch 3:  For the train data Loss: 0.125359 Acc: 0.950000\n",
      "For the validation data Loss: 1.029955 Acc: 0.730600\n",
      "Epoch 69, CIFAR-10 Batch 4:  For the train data Loss: 0.135478 Acc: 0.975000\n",
      "For the validation data Loss: 0.983501 Acc: 0.733400\n",
      "Epoch 69, CIFAR-10 Batch 5:  For the train data Loss: 0.101328 Acc: 0.975000\n",
      "For the validation data Loss: 1.020059 Acc: 0.731200\n",
      "Epoch 70, CIFAR-10 Batch 1:  For the train data Loss: 0.204563 Acc: 0.975000\n",
      "For the validation data Loss: 0.997584 Acc: 0.732800\n",
      "Epoch 70, CIFAR-10 Batch 2:  For the train data Loss: 0.114327 Acc: 0.975000\n",
      "For the validation data Loss: 0.952381 Acc: 0.740800\n",
      "Epoch 70, CIFAR-10 Batch 3:  For the train data Loss: 0.064942 Acc: 1.000000\n",
      "For the validation data Loss: 0.981960 Acc: 0.732800\n",
      "Epoch 70, CIFAR-10 Batch 4:  For the train data Loss: 0.094801 Acc: 0.975000\n",
      "For the validation data Loss: 1.002155 Acc: 0.733800\n",
      "Epoch 70, CIFAR-10 Batch 5:  For the train data Loss: 0.100963 Acc: 0.975000\n",
      "For the validation data Loss: 1.021697 Acc: 0.735400\n",
      "Epoch 71, CIFAR-10 Batch 1:  For the train data Loss: 0.241797 Acc: 0.950000\n",
      "For the validation data Loss: 1.056586 Acc: 0.730600\n",
      "Epoch 71, CIFAR-10 Batch 2:  For the train data Loss: 0.156531 Acc: 0.975000\n",
      "For the validation data Loss: 1.008301 Acc: 0.729800\n",
      "Epoch 71, CIFAR-10 Batch 3:  For the train data Loss: 0.092496 Acc: 0.975000\n",
      "For the validation data Loss: 0.998625 Acc: 0.727000\n",
      "Epoch 71, CIFAR-10 Batch 4:  For the train data Loss: 0.097452 Acc: 0.975000\n",
      "For the validation data Loss: 0.992654 Acc: 0.736200\n",
      "Epoch 71, CIFAR-10 Batch 5:  For the train data Loss: 0.074730 Acc: 1.000000\n",
      "For the validation data Loss: 1.036264 Acc: 0.728800\n",
      "Epoch 72, CIFAR-10 Batch 1:  For the train data Loss: 0.182454 Acc: 0.950000\n",
      "For the validation data Loss: 1.021549 Acc: 0.728800\n",
      "Epoch 72, CIFAR-10 Batch 2:  For the train data Loss: 0.126219 Acc: 0.975000\n",
      "For the validation data Loss: 1.050812 Acc: 0.729400\n",
      "Epoch 72, CIFAR-10 Batch 3:  For the train data Loss: 0.136505 Acc: 0.975000\n",
      "For the validation data Loss: 1.018181 Acc: 0.734000\n",
      "Epoch 72, CIFAR-10 Batch 4:  For the train data Loss: 0.091749 Acc: 0.975000\n",
      "For the validation data Loss: 1.011851 Acc: 0.734200\n",
      "Epoch 72, CIFAR-10 Batch 5:  For the train data Loss: 0.048248 Acc: 1.000000\n",
      "For the validation data Loss: 0.998908 Acc: 0.738000\n",
      "Epoch 73, CIFAR-10 Batch 1:  For the train data Loss: 0.119089 Acc: 1.000000\n",
      "For the validation data Loss: 1.016251 Acc: 0.736600\n",
      "Epoch 73, CIFAR-10 Batch 2:  For the train data Loss: 0.108999 Acc: 0.975000\n",
      "For the validation data Loss: 0.996015 Acc: 0.732600\n",
      "Epoch 73, CIFAR-10 Batch 3:  For the train data Loss: 0.093728 Acc: 1.000000\n",
      "For the validation data Loss: 0.989219 Acc: 0.735400\n",
      "Epoch 73, CIFAR-10 Batch 4:  For the train data Loss: 0.092294 Acc: 0.950000\n",
      "For the validation data Loss: 0.960776 Acc: 0.738000\n",
      "Epoch 73, CIFAR-10 Batch 5:  For the train data Loss: 0.062825 Acc: 1.000000\n",
      "For the validation data Loss: 1.009288 Acc: 0.736000\n",
      "Epoch 74, CIFAR-10 Batch 1:  For the train data Loss: 0.140098 Acc: 0.975000\n",
      "For the validation data Loss: 1.029343 Acc: 0.720400\n",
      "Epoch 74, CIFAR-10 Batch 2:  For the train data Loss: 0.123977 Acc: 0.975000\n",
      "For the validation data Loss: 1.021068 Acc: 0.732400\n",
      "Epoch 74, CIFAR-10 Batch 3:  For the train data Loss: 0.060756 Acc: 1.000000\n",
      "For the validation data Loss: 1.024626 Acc: 0.732200\n",
      "Epoch 74, CIFAR-10 Batch 4:  For the train data Loss: 0.155613 Acc: 0.950000\n",
      "For the validation data Loss: 0.981546 Acc: 0.734200\n",
      "Epoch 74, CIFAR-10 Batch 5:  For the train data Loss: 0.069101 Acc: 1.000000\n",
      "For the validation data Loss: 0.980784 Acc: 0.739000\n",
      "Epoch 75, CIFAR-10 Batch 1:  For the train data Loss: 0.154772 Acc: 0.950000\n",
      "For the validation data Loss: 1.045897 Acc: 0.739800\n",
      "Epoch 75, CIFAR-10 Batch 2:  For the train data Loss: 0.177186 Acc: 0.950000\n",
      "For the validation data Loss: 1.018701 Acc: 0.739000\n",
      "Epoch 75, CIFAR-10 Batch 3:  For the train data Loss: 0.082195 Acc: 1.000000\n",
      "For the validation data Loss: 1.041460 Acc: 0.735400\n",
      "Epoch 75, CIFAR-10 Batch 4:  For the train data Loss: 0.122753 Acc: 0.975000\n",
      "For the validation data Loss: 1.005760 Acc: 0.723400\n",
      "Epoch 75, CIFAR-10 Batch 5:  For the train data Loss: 0.077573 Acc: 1.000000\n",
      "For the validation data Loss: 0.964456 Acc: 0.739800\n",
      "Epoch 76, CIFAR-10 Batch 1:  For the train data Loss: 0.151121 Acc: 0.950000\n",
      "For the validation data Loss: 1.040269 Acc: 0.738200\n",
      "Epoch 76, CIFAR-10 Batch 2:  For the train data Loss: 0.129606 Acc: 0.950000\n",
      "For the validation data Loss: 1.028306 Acc: 0.738800\n",
      "Epoch 76, CIFAR-10 Batch 3:  For the train data Loss: 0.111725 Acc: 1.000000\n",
      "For the validation data Loss: 1.015236 Acc: 0.733800\n",
      "Epoch 76, CIFAR-10 Batch 4:  For the train data Loss: 0.133384 Acc: 0.975000\n",
      "For the validation data Loss: 1.045527 Acc: 0.735600\n",
      "Epoch 76, CIFAR-10 Batch 5:  For the train data Loss: 0.092907 Acc: 1.000000\n",
      "For the validation data Loss: 1.072382 Acc: 0.722800\n",
      "Epoch 77, CIFAR-10 Batch 1:  For the train data Loss: 0.129949 Acc: 0.950000\n",
      "For the validation data Loss: 1.066014 Acc: 0.734800\n",
      "Epoch 77, CIFAR-10 Batch 2:  For the train data Loss: 0.098547 Acc: 0.975000\n",
      "For the validation data Loss: 1.001939 Acc: 0.736000\n",
      "Epoch 77, CIFAR-10 Batch 3:  For the train data Loss: 0.099558 Acc: 1.000000\n",
      "For the validation data Loss: 1.010168 Acc: 0.732800\n",
      "Epoch 77, CIFAR-10 Batch 4:  For the train data Loss: 0.082671 Acc: 0.975000\n",
      "For the validation data Loss: 1.020813 Acc: 0.735400\n",
      "Epoch 77, CIFAR-10 Batch 5:  For the train data Loss: 0.102892 Acc: 0.975000\n",
      "For the validation data Loss: 1.078207 Acc: 0.731400\n",
      "Epoch 78, CIFAR-10 Batch 1:  For the train data Loss: 0.147257 Acc: 0.925000\n",
      "For the validation data Loss: 1.084033 Acc: 0.729800\n",
      "Epoch 78, CIFAR-10 Batch 2:  For the train data Loss: 0.125301 Acc: 0.975000\n",
      "For the validation data Loss: 1.047910 Acc: 0.725800\n",
      "Epoch 78, CIFAR-10 Batch 3:  For the train data Loss: 0.072647 Acc: 1.000000\n",
      "For the validation data Loss: 1.053928 Acc: 0.726200\n",
      "Epoch 78, CIFAR-10 Batch 4:  For the train data Loss: 0.104709 Acc: 0.975000\n",
      "For the validation data Loss: 1.038106 Acc: 0.736200\n",
      "Epoch 78, CIFAR-10 Batch 5:  For the train data Loss: 0.128442 Acc: 0.975000\n",
      "For the validation data Loss: 0.991431 Acc: 0.736400\n",
      "Epoch 79, CIFAR-10 Batch 1:  For the train data Loss: 0.081654 Acc: 1.000000\n",
      "For the validation data Loss: 1.040064 Acc: 0.737800\n",
      "Epoch 79, CIFAR-10 Batch 2:  For the train data Loss: 0.102399 Acc: 0.975000\n",
      "For the validation data Loss: 0.993928 Acc: 0.734000\n",
      "Epoch 79, CIFAR-10 Batch 3:  For the train data Loss: 0.073197 Acc: 1.000000\n",
      "For the validation data Loss: 1.041089 Acc: 0.727000\n",
      "Epoch 79, CIFAR-10 Batch 4:  For the train data Loss: 0.089516 Acc: 0.975000\n",
      "For the validation data Loss: 1.016695 Acc: 0.741800\n",
      "Epoch 79, CIFAR-10 Batch 5:  For the train data Loss: 0.081388 Acc: 1.000000\n",
      "For the validation data Loss: 0.988857 Acc: 0.739200\n",
      "Epoch 80, CIFAR-10 Batch 1:  For the train data Loss: 0.096372 Acc: 1.000000\n",
      "For the validation data Loss: 1.086337 Acc: 0.732600\n",
      "Epoch 80, CIFAR-10 Batch 2:  For the train data Loss: 0.119203 Acc: 0.950000\n",
      "For the validation data Loss: 1.001188 Acc: 0.731800\n",
      "Epoch 80, CIFAR-10 Batch 3:  For the train data Loss: 0.082412 Acc: 0.975000\n",
      "For the validation data Loss: 1.037617 Acc: 0.731600\n",
      "Epoch 80, CIFAR-10 Batch 4:  For the train data Loss: 0.076780 Acc: 1.000000\n",
      "For the validation data Loss: 0.972126 Acc: 0.734600\n",
      "Epoch 80, CIFAR-10 Batch 5:  For the train data Loss: 0.068443 Acc: 1.000000\n",
      "For the validation data Loss: 1.022288 Acc: 0.735600\n",
      "Epoch 81, CIFAR-10 Batch 1:  For the train data Loss: 0.126735 Acc: 1.000000\n",
      "For the validation data Loss: 1.095648 Acc: 0.732200\n",
      "Epoch 81, CIFAR-10 Batch 2:  For the train data Loss: 0.089658 Acc: 1.000000\n",
      "For the validation data Loss: 0.997868 Acc: 0.727200\n",
      "Epoch 81, CIFAR-10 Batch 3:  For the train data Loss: 0.091000 Acc: 1.000000\n",
      "For the validation data Loss: 1.026681 Acc: 0.735600\n",
      "Epoch 81, CIFAR-10 Batch 4:  For the train data Loss: 0.077743 Acc: 0.975000\n",
      "For the validation data Loss: 1.031614 Acc: 0.739400\n",
      "Epoch 81, CIFAR-10 Batch 5:  For the train data Loss: 0.056430 Acc: 1.000000\n",
      "For the validation data Loss: 1.053452 Acc: 0.729400\n",
      "Epoch 82, CIFAR-10 Batch 1:  For the train data Loss: 0.117332 Acc: 0.975000\n",
      "For the validation data Loss: 1.035344 Acc: 0.734600\n",
      "Epoch 82, CIFAR-10 Batch 2:  For the train data Loss: 0.113151 Acc: 1.000000\n",
      "For the validation data Loss: 1.053194 Acc: 0.736800\n",
      "Epoch 82, CIFAR-10 Batch 3:  For the train data Loss: 0.089713 Acc: 1.000000\n",
      "For the validation data Loss: 1.034371 Acc: 0.735400\n",
      "Epoch 82, CIFAR-10 Batch 4:  For the train data Loss: 0.104748 Acc: 0.975000\n",
      "For the validation data Loss: 1.015639 Acc: 0.740800\n",
      "Epoch 82, CIFAR-10 Batch 5:  For the train data Loss: 0.055739 Acc: 1.000000\n",
      "For the validation data Loss: 1.115688 Acc: 0.732800\n",
      "Epoch 83, CIFAR-10 Batch 1:  For the train data Loss: 0.117259 Acc: 0.975000\n",
      "For the validation data Loss: 1.116008 Acc: 0.731000\n",
      "Epoch 83, CIFAR-10 Batch 2:  For the train data Loss: 0.084943 Acc: 0.975000\n",
      "For the validation data Loss: 1.083237 Acc: 0.731200\n",
      "Epoch 83, CIFAR-10 Batch 3:  For the train data Loss: 0.108816 Acc: 1.000000\n",
      "For the validation data Loss: 1.039922 Acc: 0.728000\n",
      "Epoch 83, CIFAR-10 Batch 4:  For the train data Loss: 0.102124 Acc: 1.000000\n",
      "For the validation data Loss: 1.011937 Acc: 0.737400\n",
      "Epoch 83, CIFAR-10 Batch 5:  For the train data Loss: 0.065095 Acc: 1.000000\n",
      "For the validation data Loss: 1.074075 Acc: 0.733600\n",
      "Epoch 84, CIFAR-10 Batch 1:  For the train data Loss: 0.114502 Acc: 0.975000\n",
      "For the validation data Loss: 1.076723 Acc: 0.734200\n",
      "Epoch 84, CIFAR-10 Batch 2:  For the train data Loss: 0.169962 Acc: 0.925000\n",
      "For the validation data Loss: 1.046722 Acc: 0.730200\n",
      "Epoch 84, CIFAR-10 Batch 3:  For the train data Loss: 0.075452 Acc: 0.975000\n",
      "For the validation data Loss: 1.067816 Acc: 0.734400\n",
      "Epoch 84, CIFAR-10 Batch 4:  For the train data Loss: 0.104910 Acc: 0.975000\n",
      "For the validation data Loss: 1.096830 Acc: 0.727200\n",
      "Epoch 84, CIFAR-10 Batch 5:  For the train data Loss: 0.076073 Acc: 1.000000\n",
      "For the validation data Loss: 1.087939 Acc: 0.727200\n",
      "Epoch 85, CIFAR-10 Batch 1:  For the train data Loss: 0.110661 Acc: 1.000000\n",
      "For the validation data Loss: 1.051462 Acc: 0.737800\n",
      "Epoch 85, CIFAR-10 Batch 2:  For the train data Loss: 0.166127 Acc: 0.975000\n",
      "For the validation data Loss: 1.089721 Acc: 0.718000\n",
      "Epoch 85, CIFAR-10 Batch 3:  For the train data Loss: 0.066918 Acc: 1.000000\n",
      "For the validation data Loss: 1.017965 Acc: 0.733200\n",
      "Epoch 85, CIFAR-10 Batch 4:  For the train data Loss: 0.098944 Acc: 0.975000\n",
      "For the validation data Loss: 1.069839 Acc: 0.725000\n",
      "Epoch 85, CIFAR-10 Batch 5:  For the train data Loss: 0.107930 Acc: 0.975000\n",
      "For the validation data Loss: 1.106949 Acc: 0.727800\n",
      "Epoch 86, CIFAR-10 Batch 1:  For the train data Loss: 0.142986 Acc: 0.975000\n",
      "For the validation data Loss: 1.093218 Acc: 0.733400\n",
      "Epoch 86, CIFAR-10 Batch 2:  For the train data Loss: 0.152411 Acc: 1.000000\n",
      "For the validation data Loss: 1.034009 Acc: 0.736600\n",
      "Epoch 86, CIFAR-10 Batch 3:  For the train data Loss: 0.113882 Acc: 0.975000\n",
      "For the validation data Loss: 1.049921 Acc: 0.730400\n",
      "Epoch 86, CIFAR-10 Batch 4:  For the train data Loss: 0.075130 Acc: 0.975000\n",
      "For the validation data Loss: 1.027134 Acc: 0.734400\n",
      "Epoch 86, CIFAR-10 Batch 5:  For the train data Loss: 0.064101 Acc: 1.000000\n",
      "For the validation data Loss: 1.107125 Acc: 0.724000\n",
      "Epoch 87, CIFAR-10 Batch 1:  For the train data Loss: 0.102067 Acc: 0.950000\n",
      "For the validation data Loss: 1.115471 Acc: 0.739400\n",
      "Epoch 87, CIFAR-10 Batch 2:  For the train data Loss: 0.102514 Acc: 0.975000\n",
      "For the validation data Loss: 1.103336 Acc: 0.728800\n",
      "Epoch 87, CIFAR-10 Batch 3:  For the train data Loss: 0.075014 Acc: 0.975000\n",
      "For the validation data Loss: 1.082117 Acc: 0.731200\n",
      "Epoch 87, CIFAR-10 Batch 4:  For the train data Loss: 0.063349 Acc: 1.000000\n",
      "For the validation data Loss: 1.103117 Acc: 0.735400\n",
      "Epoch 87, CIFAR-10 Batch 5:  For the train data Loss: 0.075599 Acc: 0.975000\n",
      "For the validation data Loss: 1.115082 Acc: 0.723400\n",
      "Epoch 88, CIFAR-10 Batch 1:  For the train data Loss: 0.110168 Acc: 1.000000\n",
      "For the validation data Loss: 1.090518 Acc: 0.731200\n",
      "Epoch 88, CIFAR-10 Batch 2:  For the train data Loss: 0.114668 Acc: 0.975000\n",
      "For the validation data Loss: 1.089521 Acc: 0.730600\n",
      "Epoch 88, CIFAR-10 Batch 3:  For the train data Loss: 0.093359 Acc: 1.000000\n",
      "For the validation data Loss: 1.097970 Acc: 0.729800\n",
      "Epoch 88, CIFAR-10 Batch 4:  For the train data Loss: 0.078780 Acc: 0.975000\n",
      "For the validation data Loss: 1.075705 Acc: 0.736000\n",
      "Epoch 88, CIFAR-10 Batch 5:  For the train data Loss: 0.117739 Acc: 0.950000\n",
      "For the validation data Loss: 1.054581 Acc: 0.731000\n",
      "Epoch 89, CIFAR-10 Batch 1:  For the train data Loss: 0.129829 Acc: 1.000000\n",
      "For the validation data Loss: 1.041433 Acc: 0.733800\n",
      "Epoch 89, CIFAR-10 Batch 2:  For the train data Loss: 0.136049 Acc: 1.000000\n",
      "For the validation data Loss: 1.081620 Acc: 0.735200\n",
      "Epoch 89, CIFAR-10 Batch 3:  For the train data Loss: 0.090437 Acc: 0.975000\n",
      "For the validation data Loss: 1.057282 Acc: 0.729800\n",
      "Epoch 89, CIFAR-10 Batch 4:  For the train data Loss: 0.082434 Acc: 0.975000\n",
      "For the validation data Loss: 1.114990 Acc: 0.737200\n",
      "Epoch 89, CIFAR-10 Batch 5:  For the train data Loss: 0.122044 Acc: 0.950000\n",
      "For the validation data Loss: 1.129489 Acc: 0.735000\n",
      "Epoch 90, CIFAR-10 Batch 1:  For the train data Loss: 0.107512 Acc: 0.975000\n",
      "For the validation data Loss: 1.075620 Acc: 0.729600\n",
      "Epoch 90, CIFAR-10 Batch 2:  For the train data Loss: 0.111697 Acc: 0.975000\n",
      "For the validation data Loss: 1.082715 Acc: 0.736600\n",
      "Epoch 90, CIFAR-10 Batch 3:  For the train data Loss: 0.078126 Acc: 1.000000\n",
      "For the validation data Loss: 1.044905 Acc: 0.733400\n",
      "Epoch 90, CIFAR-10 Batch 4:  For the train data Loss: 0.129067 Acc: 0.975000\n",
      "For the validation data Loss: 1.066186 Acc: 0.741400\n",
      "Epoch 90, CIFAR-10 Batch 5:  For the train data Loss: 0.042673 Acc: 1.000000\n",
      "For the validation data Loss: 1.124376 Acc: 0.724400\n",
      "Epoch 91, CIFAR-10 Batch 1:  For the train data Loss: 0.074979 Acc: 1.000000\n",
      "For the validation data Loss: 1.186911 Acc: 0.726600\n",
      "Epoch 91, CIFAR-10 Batch 2:  For the train data Loss: 0.079615 Acc: 1.000000\n",
      "For the validation data Loss: 1.138342 Acc: 0.730400\n",
      "Epoch 91, CIFAR-10 Batch 3:  For the train data Loss: 0.072411 Acc: 1.000000\n",
      "For the validation data Loss: 1.093717 Acc: 0.735200\n",
      "Epoch 91, CIFAR-10 Batch 4:  For the train data Loss: 0.099678 Acc: 1.000000\n",
      "For the validation data Loss: 1.078574 Acc: 0.732200\n",
      "Epoch 91, CIFAR-10 Batch 5:  For the train data Loss: 0.061978 Acc: 1.000000\n",
      "For the validation data Loss: 1.096623 Acc: 0.736200\n",
      "Epoch 92, CIFAR-10 Batch 1:  For the train data Loss: 0.126616 Acc: 1.000000\n",
      "For the validation data Loss: 1.120302 Acc: 0.724000\n",
      "Epoch 92, CIFAR-10 Batch 2:  For the train data Loss: 0.177957 Acc: 0.950000\n",
      "For the validation data Loss: 1.130300 Acc: 0.730400\n",
      "Epoch 92, CIFAR-10 Batch 3:  For the train data Loss: 0.042869 Acc: 1.000000\n",
      "For the validation data Loss: 1.052168 Acc: 0.733200\n",
      "Epoch 92, CIFAR-10 Batch 4:  For the train data Loss: 0.073724 Acc: 1.000000\n",
      "For the validation data Loss: 1.112172 Acc: 0.735200\n",
      "Epoch 92, CIFAR-10 Batch 5:  For the train data Loss: 0.077270 Acc: 1.000000\n",
      "For the validation data Loss: 1.134153 Acc: 0.729200\n",
      "Epoch 93, CIFAR-10 Batch 1:  For the train data Loss: 0.103942 Acc: 0.975000\n",
      "For the validation data Loss: 1.115420 Acc: 0.730000\n",
      "Epoch 93, CIFAR-10 Batch 2:  For the train data Loss: 0.164362 Acc: 0.975000\n",
      "For the validation data Loss: 1.181092 Acc: 0.719800\n",
      "Epoch 93, CIFAR-10 Batch 3:  For the train data Loss: 0.099716 Acc: 1.000000\n",
      "For the validation data Loss: 1.119616 Acc: 0.733600\n",
      "Epoch 93, CIFAR-10 Batch 4:  For the train data Loss: 0.083166 Acc: 0.975000\n",
      "For the validation data Loss: 1.124185 Acc: 0.732000\n",
      "Epoch 93, CIFAR-10 Batch 5:  For the train data Loss: 0.083518 Acc: 1.000000\n",
      "For the validation data Loss: 1.114160 Acc: 0.722600\n",
      "Epoch 94, CIFAR-10 Batch 1:  For the train data Loss: 0.125408 Acc: 0.950000\n",
      "For the validation data Loss: 1.079631 Acc: 0.735200\n",
      "Epoch 94, CIFAR-10 Batch 2:  For the train data Loss: 0.105042 Acc: 1.000000\n",
      "For the validation data Loss: 1.115709 Acc: 0.719800\n",
      "Epoch 94, CIFAR-10 Batch 3:  For the train data Loss: 0.071201 Acc: 1.000000\n",
      "For the validation data Loss: 1.117900 Acc: 0.730200\n",
      "Epoch 94, CIFAR-10 Batch 4:  For the train data Loss: 0.117891 Acc: 0.950000\n",
      "For the validation data Loss: 1.078396 Acc: 0.729400\n",
      "Epoch 94, CIFAR-10 Batch 5:  For the train data Loss: 0.072572 Acc: 1.000000\n",
      "For the validation data Loss: 1.090989 Acc: 0.728600\n",
      "Epoch 95, CIFAR-10 Batch 1:  For the train data Loss: 0.109111 Acc: 0.975000\n",
      "For the validation data Loss: 1.165040 Acc: 0.725800\n",
      "Epoch 95, CIFAR-10 Batch 2:  For the train data Loss: 0.108897 Acc: 0.975000\n",
      "For the validation data Loss: 1.107826 Acc: 0.727200\n",
      "Epoch 95, CIFAR-10 Batch 3:  For the train data Loss: 0.080133 Acc: 0.975000\n",
      "For the validation data Loss: 1.117437 Acc: 0.733400\n",
      "Epoch 95, CIFAR-10 Batch 4:  For the train data Loss: 0.077753 Acc: 0.975000\n",
      "For the validation data Loss: 1.097352 Acc: 0.733400\n",
      "Epoch 95, CIFAR-10 Batch 5:  For the train data Loss: 0.063152 Acc: 0.975000\n",
      "For the validation data Loss: 1.171282 Acc: 0.725200\n",
      "Epoch 96, CIFAR-10 Batch 1:  For the train data Loss: 0.117304 Acc: 0.975000\n",
      "For the validation data Loss: 1.130585 Acc: 0.725400\n",
      "Epoch 96, CIFAR-10 Batch 2:  For the train data Loss: 0.108977 Acc: 1.000000\n",
      "For the validation data Loss: 1.156006 Acc: 0.722800\n",
      "Epoch 96, CIFAR-10 Batch 3:  For the train data Loss: 0.059719 Acc: 1.000000\n",
      "For the validation data Loss: 1.097425 Acc: 0.731800\n",
      "Epoch 96, CIFAR-10 Batch 4:  For the train data Loss: 0.071987 Acc: 1.000000\n",
      "For the validation data Loss: 1.078088 Acc: 0.728400\n",
      "Epoch 96, CIFAR-10 Batch 5:  For the train data Loss: 0.056528 Acc: 1.000000\n",
      "For the validation data Loss: 1.111997 Acc: 0.731200\n",
      "Epoch 97, CIFAR-10 Batch 1:  For the train data Loss: 0.086414 Acc: 1.000000\n",
      "For the validation data Loss: 1.121779 Acc: 0.723400\n",
      "Epoch 97, CIFAR-10 Batch 2:  For the train data Loss: 0.100006 Acc: 1.000000\n",
      "For the validation data Loss: 1.154518 Acc: 0.723600\n",
      "Epoch 97, CIFAR-10 Batch 3:  For the train data Loss: 0.037886 Acc: 1.000000\n",
      "For the validation data Loss: 1.098783 Acc: 0.726400\n",
      "Epoch 97, CIFAR-10 Batch 4:  For the train data Loss: 0.098881 Acc: 0.975000\n",
      "For the validation data Loss: 1.095593 Acc: 0.724600\n",
      "Epoch 97, CIFAR-10 Batch 5:  For the train data Loss: 0.063024 Acc: 1.000000\n",
      "For the validation data Loss: 1.136945 Acc: 0.734600\n",
      "Epoch 98, CIFAR-10 Batch 1:  For the train data Loss: 0.172742 Acc: 0.925000\n",
      "For the validation data Loss: 1.157197 Acc: 0.725400\n",
      "Epoch 98, CIFAR-10 Batch 2:  For the train data Loss: 0.130730 Acc: 0.975000\n",
      "For the validation data Loss: 1.105671 Acc: 0.728600\n",
      "Epoch 98, CIFAR-10 Batch 3:  For the train data Loss: 0.057883 Acc: 1.000000\n",
      "For the validation data Loss: 1.153054 Acc: 0.718600\n",
      "Epoch 98, CIFAR-10 Batch 4:  For the train data Loss: 0.066671 Acc: 1.000000\n",
      "For the validation data Loss: 1.046888 Acc: 0.734200\n",
      "Epoch 98, CIFAR-10 Batch 5:  For the train data Loss: 0.112428 Acc: 0.975000\n",
      "For the validation data Loss: 1.082503 Acc: 0.726000\n",
      "Epoch 99, CIFAR-10 Batch 1:  For the train data Loss: 0.112598 Acc: 0.975000\n",
      "For the validation data Loss: 1.147115 Acc: 0.735800\n",
      "Epoch 99, CIFAR-10 Batch 2:  For the train data Loss: 0.125430 Acc: 0.975000\n",
      "For the validation data Loss: 1.100608 Acc: 0.726400\n",
      "Epoch 99, CIFAR-10 Batch 3:  For the train data Loss: 0.088958 Acc: 0.975000\n",
      "For the validation data Loss: 1.143204 Acc: 0.735400\n",
      "Epoch 99, CIFAR-10 Batch 4:  For the train data Loss: 0.067736 Acc: 0.975000\n",
      "For the validation data Loss: 1.100994 Acc: 0.729200\n",
      "Epoch 99, CIFAR-10 Batch 5:  For the train data Loss: 0.068896 Acc: 0.975000\n",
      "For the validation data Loss: 1.169751 Acc: 0.738200\n",
      "Epoch 100, CIFAR-10 Batch 1:  For the train data Loss: 0.131495 Acc: 1.000000\n",
      "For the validation data Loss: 1.126912 Acc: 0.728000\n",
      "Epoch 100, CIFAR-10 Batch 2:  For the train data Loss: 0.083767 Acc: 1.000000\n",
      "For the validation data Loss: 1.149225 Acc: 0.723400\n",
      "Epoch 100, CIFAR-10 Batch 3:  For the train data Loss: 0.056789 Acc: 1.000000\n",
      "For the validation data Loss: 1.146895 Acc: 0.728800\n",
      "Epoch 100, CIFAR-10 Batch 4:  For the train data Loss: 0.067708 Acc: 0.975000\n",
      "For the validation data Loss: 1.170831 Acc: 0.734600\n",
      "Epoch 100, CIFAR-10 Batch 5:  For the train data Loss: 0.040258 Acc: 1.000000\n",
      "For the validation data Loss: 1.121740 Acc: 0.740600\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7250199044585988\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f10cca3b2e8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
